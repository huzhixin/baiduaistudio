{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 基于paddlenlp的文本相似度判断\n",
    "\n",
    "使用paddlenlp预训练模型进行文本相似度判断，自然语言处理起步！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 项目背景\n",
    "\n",
    "在新闻媒体行业中，通常需要处理大量的文本内容，对于新闻编辑文本处理工作是一项繁重的工作，本文作为自然语言处理起步训练，主要是用paddlenlp预训练模型完成识别工作，主要体验paddlepaddle进行深度学习的完整步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据集\n",
    "\n",
    "本文作为起步文章，使用千言文本相似度判断数据集，数据包括100000行训练数据和10000行预测数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用的套件：使用了 PaddleNlp\n",
    "\n",
    "使用了什么优化器：使用了 AdamW\n",
    "\n",
    "调整了那些参数：\n",
    "（比如batchsize设置多少、是否使用warmup、base lr是否调整、是否尝试lr stepdecay等，这样做是否有提升等）\n",
    "learning_rate\n",
    "\n",
    "心得：paddlenlp提供了常用的预训练模型，对普通开发者友好，pytorch用户可以快速转换paddle。\n",
    "\n",
    "科大讯飞NLP算法赛baseline：学术论文分类挑战赛\n",
    "https://aistudio.baidu.com/aistudio/projectdetail/2201232"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 安装paddlenlp包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 完整训练代码\r\n",
    "!pip install -U -q paddlenlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据预处理\n",
    "数据集解压和数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "#!unzip -qo /home/aistudio/data/bq_corpus.zip -d /home/aistudio/dataset\n",
    "#!rm -rf /home/aistudio/dataset/__MACOSX/\n",
    "from paddlenlp.datasets import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取文件数据\n",
    "def get_data(path, is_test=False):\n",
    "    dataset_ = []\n",
    "    \n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        if is_test:\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                tmp = line.split(\"\\t\")\n",
    "                dataset_.append({\"title\": tmp[0], \"paire\": tmp[1]})\n",
    "        else:\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                tmp = line.split(\"\\t\")\n",
    "                dataset_.append({\"title\": tmp[0], \"paire\": tmp[1], \"label\": int(tmp[2])})\n",
    "    \n",
    "    dataset_ = dataset.MapDataset(dataset_)\n",
    "    \n",
    "    return dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ = get_data('/home/aistudio/dataset/bq_corpus/train.tsv')\n",
    "dev_ = get_data('/home/aistudio/dataset/bq_corpus/dev.tsv')\n",
    "test_ = get_data('/home/aistudio/dataset/bq_corpus/test.tsv', is_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 导入所需包并定义与训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导入python包\n",
    "import paddlenlp as pdnlp\n",
    "from paddlenlp.data import Pad, Tuple, Stack\n",
    "from paddlenlp.transformers import ErnieGramTokenizer, ErnieGramForSequenceClassification\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-08-15 17:50:36,805] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie_gram_zh/ernie_gram_zh.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-gram-zh\n",
      "[2021-08-15 17:50:36,873] [    INFO] - Downloading ernie_gram_zh.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/ernie_gram_zh/ernie_gram_zh.pdparams\n",
      "100%|██████████| 583566/583566 [00:24<00:00, 24009.16it/s]\n",
      "[2021-08-15 17:51:17,041] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/ernie_gram_zh/vocab.txt\n",
      "100%|██████████| 78/78 [00:00<00:00, 2579.60it/s]\n"
     ]
    }
   ],
   "source": [
    "ernie_model = ErnieGramForSequenceClassification.from_pretrained(\"ernie-gram-zh\", num_classes=2)\n",
    "ernie_tokenizer = ErnieGramTokenizer.from_pretrained(\"ernie-gram-zh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 定于数据处理方法\n",
    "\n",
    "数据处理主要是将文字转为ID，生成paddlepaddle可以处理的批量数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将文字转换为ID表示\n",
    "# 本文尝试将文本内容分别截取之后再拼接，避免超长语句导致第二句未被取到的情况\n",
    "def convert_words(line, tokenizer, max_len, is_test=False):\n",
    "    half_len = int(max_len/2)\n",
    "    title = line['title'][:half_len]\n",
    "    paire = line['paire'][:half_len]\n",
    "\n",
    "    contact_line = title + paire\n",
    "    encoded_line = tokenizer(contact_line, max_seq_len=max_len)\n",
    "    \n",
    "    input_ids = encoded_line['input_ids']\n",
    "    token_type_ids = encoded_line['token_type_ids']\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([line['label']])\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_seq_length = 128\n",
    "\n",
    "trans_func = partial(\n",
    "    convert_words,\n",
    "    tokenizer=ernie_tokenizer,\n",
    "    max_len=max_seq_length)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=ernie_tokenizer.pad_token_id),       # input\n",
    "    Pad(axis=0, pad_val=ernie_tokenizer.pad_token_type_id),  # segment\n",
    "    Stack(dtype=\"int64\")                               # label\n",
    "): [data for data in fn(samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建批量数据迭代器\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_loader = create_dataloader(\n",
    "    train_,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 定义神经网络需要的参数\n",
    "\n",
    "主要是学习率、优化器、训练轮数等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "# 训练过程中的最大学习率\n",
    "learning_rate = 5e-5\n",
    "# 训练轮次\n",
    "epochs = 12\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=ernie_model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 定义评估函数 用于对训练效果进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 进行深度网络训练并保存训练模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 100, epoch: 1, batch: 100, loss: 0.71411, acc: 0.49984\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.65717, acc: 0.52219\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.49822, acc: 0.55984\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.46257, acc: 0.60328\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.47058, acc: 0.63500\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.54045, acc: 0.65846\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.44414, acc: 0.67699\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.36696, acc: 0.69078\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.40430, acc: 0.70462\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.32117, acc: 0.71609\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.45164, acc: 0.72503\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.43935, acc: 0.73393\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.40799, acc: 0.74214\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.37259, acc: 0.74837\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.36657, acc: 0.75443\n",
      "eval loss: 0.39729, accu: 0.83090\n",
      "global step 1600, epoch: 2, batch: 37, loss: 0.58716, acc: 0.85346\n",
      "global step 1700, epoch: 2, batch: 137, loss: 0.28784, acc: 0.85709\n",
      "global step 1800, epoch: 2, batch: 237, loss: 0.48710, acc: 0.85410\n",
      "global step 1900, epoch: 2, batch: 337, loss: 0.41151, acc: 0.85478\n",
      "global step 2000, epoch: 2, batch: 437, loss: 0.20951, acc: 0.85762\n",
      "global step 2100, epoch: 2, batch: 537, loss: 0.39031, acc: 0.85670\n",
      "global step 2200, epoch: 2, batch: 637, loss: 0.22553, acc: 0.85771\n",
      "global step 2300, epoch: 2, batch: 737, loss: 0.40812, acc: 0.85846\n",
      "global step 2400, epoch: 2, batch: 837, loss: 0.34548, acc: 0.85932\n",
      "global step 2500, epoch: 2, batch: 937, loss: 0.21683, acc: 0.86116\n",
      "global step 2600, epoch: 2, batch: 1037, loss: 0.26971, acc: 0.86222\n",
      "global step 2700, epoch: 2, batch: 1137, loss: 0.25350, acc: 0.86354\n",
      "global step 2800, epoch: 2, batch: 1237, loss: 0.35235, acc: 0.86467\n",
      "global step 2900, epoch: 2, batch: 1337, loss: 0.28091, acc: 0.86550\n",
      "global step 3000, epoch: 2, batch: 1437, loss: 0.28676, acc: 0.86682\n",
      "global step 3100, epoch: 2, batch: 1537, loss: 0.28191, acc: 0.86843\n",
      "eval loss: 0.43132, accu: 0.84880\n",
      "global step 3200, epoch: 3, batch: 74, loss: 0.25021, acc: 0.91280\n",
      "global step 3300, epoch: 3, batch: 174, loss: 0.37185, acc: 0.90948\n",
      "global step 3400, epoch: 3, batch: 274, loss: 0.20239, acc: 0.90842\n",
      "global step 3500, epoch: 3, batch: 374, loss: 0.11662, acc: 0.90934\n",
      "global step 3600, epoch: 3, batch: 474, loss: 0.27312, acc: 0.91100\n",
      "global step 3700, epoch: 3, batch: 574, loss: 0.25273, acc: 0.91123\n",
      "global step 3800, epoch: 3, batch: 674, loss: 0.37508, acc: 0.91165\n",
      "global step 3900, epoch: 3, batch: 774, loss: 0.23911, acc: 0.91085\n",
      "global step 4000, epoch: 3, batch: 874, loss: 0.16440, acc: 0.91126\n",
      "global step 4100, epoch: 3, batch: 974, loss: 0.21821, acc: 0.91109\n",
      "global step 4200, epoch: 3, batch: 1074, loss: 0.19521, acc: 0.91172\n",
      "global step 4300, epoch: 3, batch: 1174, loss: 0.18893, acc: 0.91191\n",
      "global step 4400, epoch: 3, batch: 1274, loss: 0.22822, acc: 0.91204\n",
      "global step 4500, epoch: 3, batch: 1374, loss: 0.11513, acc: 0.91271\n",
      "global step 4600, epoch: 3, batch: 1474, loss: 0.37578, acc: 0.91296\n",
      "eval loss: 0.40454, accu: 0.84500\n",
      "global step 4700, epoch: 4, batch: 11, loss: 0.08886, acc: 0.93324\n",
      "global step 4800, epoch: 4, batch: 111, loss: 0.13158, acc: 0.94412\n",
      "global step 4900, epoch: 4, batch: 211, loss: 0.06003, acc: 0.94357\n",
      "global step 5000, epoch: 4, batch: 311, loss: 0.22469, acc: 0.94232\n",
      "global step 5100, epoch: 4, batch: 411, loss: 0.22660, acc: 0.94134\n",
      "global step 5200, epoch: 4, batch: 511, loss: 0.15197, acc: 0.94016\n",
      "global step 5300, epoch: 4, batch: 611, loss: 0.25733, acc: 0.93988\n",
      "global step 5400, epoch: 4, batch: 711, loss: 0.17498, acc: 0.93915\n",
      "global step 5500, epoch: 4, batch: 811, loss: 0.05907, acc: 0.93908\n",
      "global step 5600, epoch: 4, batch: 911, loss: 0.14121, acc: 0.93932\n",
      "global step 5700, epoch: 4, batch: 1011, loss: 0.17649, acc: 0.93929\n",
      "global step 5800, epoch: 4, batch: 1111, loss: 0.24345, acc: 0.93986\n",
      "global step 5900, epoch: 4, batch: 1211, loss: 0.11687, acc: 0.93987\n",
      "global step 6000, epoch: 4, batch: 1311, loss: 0.12769, acc: 0.94001\n",
      "global step 6100, epoch: 4, batch: 1411, loss: 0.33753, acc: 0.93994\n",
      "global step 6200, epoch: 4, batch: 1511, loss: 0.24538, acc: 0.94000\n",
      "eval loss: 0.46818, accu: 0.84410\n",
      "global step 6300, epoch: 5, batch: 48, loss: 0.13266, acc: 0.96191\n",
      "global step 6400, epoch: 5, batch: 148, loss: 0.10195, acc: 0.95978\n",
      "global step 6500, epoch: 5, batch: 248, loss: 0.21553, acc: 0.95955\n",
      "global step 6600, epoch: 5, batch: 348, loss: 0.04148, acc: 0.95874\n",
      "global step 6700, epoch: 5, batch: 448, loss: 0.13573, acc: 0.95728\n",
      "global step 6800, epoch: 5, batch: 548, loss: 0.04680, acc: 0.95743\n",
      "global step 6900, epoch: 5, batch: 648, loss: 0.22762, acc: 0.95713\n",
      "global step 7000, epoch: 5, batch: 748, loss: 0.09687, acc: 0.95670\n",
      "global step 7100, epoch: 5, batch: 848, loss: 0.08694, acc: 0.95723\n",
      "global step 7200, epoch: 5, batch: 948, loss: 0.03454, acc: 0.95690\n",
      "global step 7300, epoch: 5, batch: 1048, loss: 0.08953, acc: 0.95711\n",
      "global step 7400, epoch: 5, batch: 1148, loss: 0.11853, acc: 0.95703\n",
      "global step 7500, epoch: 5, batch: 1248, loss: 0.26757, acc: 0.95696\n",
      "global step 7600, epoch: 5, batch: 1348, loss: 0.05621, acc: 0.95736\n",
      "global step 7700, epoch: 5, batch: 1448, loss: 0.07734, acc: 0.95743\n",
      "global step 7800, epoch: 5, batch: 1548, loss: 0.09874, acc: 0.95730\n",
      "eval loss: 0.53441, accu: 0.84750\n",
      "global step 7900, epoch: 6, batch: 85, loss: 0.04767, acc: 0.97224\n",
      "global step 8000, epoch: 6, batch: 185, loss: 0.10344, acc: 0.97095\n",
      "global step 8100, epoch: 6, batch: 285, loss: 0.02921, acc: 0.97083\n",
      "global step 8200, epoch: 6, batch: 385, loss: 0.11573, acc: 0.97029\n",
      "global step 8300, epoch: 6, batch: 485, loss: 0.01605, acc: 0.97049\n",
      "global step 8400, epoch: 6, batch: 585, loss: 0.06620, acc: 0.97094\n",
      "global step 8500, epoch: 6, batch: 685, loss: 0.13095, acc: 0.97003\n",
      "global step 8600, epoch: 6, batch: 785, loss: 0.03267, acc: 0.97006\n",
      "global step 8700, epoch: 6, batch: 885, loss: 0.11968, acc: 0.96983\n",
      "global step 8800, epoch: 6, batch: 985, loss: 0.03655, acc: 0.96972\n",
      "global step 8900, epoch: 6, batch: 1085, loss: 0.11208, acc: 0.96963\n",
      "global step 9000, epoch: 6, batch: 1185, loss: 0.08050, acc: 0.96973\n",
      "global step 9100, epoch: 6, batch: 1285, loss: 0.02985, acc: 0.96967\n",
      "global step 9200, epoch: 6, batch: 1385, loss: 0.16350, acc: 0.96963\n",
      "global step 9300, epoch: 6, batch: 1485, loss: 0.14760, acc: 0.96947\n",
      "eval loss: 0.58101, accu: 0.84970\n",
      "global step 9400, epoch: 7, batch: 22, loss: 0.02443, acc: 0.97727\n",
      "global step 9500, epoch: 7, batch: 122, loss: 0.02525, acc: 0.98156\n",
      "global step 9600, epoch: 7, batch: 222, loss: 0.01470, acc: 0.98170\n",
      "global step 9700, epoch: 7, batch: 322, loss: 0.02575, acc: 0.98064\n",
      "global step 9800, epoch: 7, batch: 422, loss: 0.06491, acc: 0.97967\n",
      "global step 9900, epoch: 7, batch: 522, loss: 0.08684, acc: 0.97944\n",
      "global step 10000, epoch: 7, batch: 622, loss: 0.06442, acc: 0.97887\n",
      "global step 10100, epoch: 7, batch: 722, loss: 0.04198, acc: 0.97853\n",
      "global step 10200, epoch: 7, batch: 822, loss: 0.10734, acc: 0.97805\n",
      "global step 10300, epoch: 7, batch: 922, loss: 0.10074, acc: 0.97802\n",
      "global step 10400, epoch: 7, batch: 1022, loss: 0.03754, acc: 0.97821\n",
      "global step 10500, epoch: 7, batch: 1122, loss: 0.01932, acc: 0.97822\n",
      "global step 10600, epoch: 7, batch: 1222, loss: 0.17488, acc: 0.97831\n",
      "global step 10700, epoch: 7, batch: 1322, loss: 0.01505, acc: 0.97849\n",
      "global step 10800, epoch: 7, batch: 1422, loss: 0.04801, acc: 0.97856\n",
      "global step 10900, epoch: 7, batch: 1522, loss: 0.01150, acc: 0.97877\n",
      "eval loss: 0.60501, accu: 0.85050\n",
      "global step 11000, epoch: 8, batch: 59, loss: 0.02369, acc: 0.98729\n",
      "global step 11100, epoch: 8, batch: 159, loss: 0.00764, acc: 0.98673\n",
      "global step 11200, epoch: 8, batch: 259, loss: 0.01969, acc: 0.98685\n",
      "global step 11300, epoch: 8, batch: 359, loss: 0.08363, acc: 0.98581\n",
      "global step 11400, epoch: 8, batch: 459, loss: 0.08417, acc: 0.98567\n",
      "global step 11500, epoch: 8, batch: 559, loss: 0.00754, acc: 0.98594\n",
      "global step 11600, epoch: 8, batch: 659, loss: 0.09959, acc: 0.98594\n",
      "global step 11700, epoch: 8, batch: 759, loss: 0.02061, acc: 0.98598\n",
      "global step 11800, epoch: 8, batch: 859, loss: 0.01044, acc: 0.98578\n",
      "global step 11900, epoch: 8, batch: 959, loss: 0.03099, acc: 0.98584\n",
      "global step 12000, epoch: 8, batch: 1059, loss: 0.01629, acc: 0.98594\n",
      "global step 12100, epoch: 8, batch: 1159, loss: 0.07977, acc: 0.98578\n",
      "global step 12200, epoch: 8, batch: 1259, loss: 0.02663, acc: 0.98572\n",
      "global step 12300, epoch: 8, batch: 1359, loss: 0.05442, acc: 0.98548\n",
      "global step 12400, epoch: 8, batch: 1459, loss: 0.05502, acc: 0.98550\n",
      "global step 12500, epoch: 8, batch: 1559, loss: 0.02518, acc: 0.98559\n",
      "eval loss: 0.71634, accu: 0.85140\n",
      "global step 12600, epoch: 9, batch: 96, loss: 0.04929, acc: 0.99202\n",
      "global step 12700, epoch: 9, batch: 196, loss: 0.00511, acc: 0.99099\n",
      "global step 12800, epoch: 9, batch: 296, loss: 0.00293, acc: 0.99045\n",
      "global step 12900, epoch: 9, batch: 396, loss: 0.00238, acc: 0.99037\n",
      "global step 13000, epoch: 9, batch: 496, loss: 0.01727, acc: 0.99008\n",
      "global step 13100, epoch: 9, batch: 596, loss: 0.00748, acc: 0.99012\n",
      "global step 13200, epoch: 9, batch: 696, loss: 0.01385, acc: 0.99021\n",
      "global step 13300, epoch: 9, batch: 796, loss: 0.00632, acc: 0.99022\n",
      "global step 13400, epoch: 9, batch: 896, loss: 0.00634, acc: 0.99029\n",
      "global step 13500, epoch: 9, batch: 996, loss: 0.05948, acc: 0.98998\n",
      "global step 13600, epoch: 9, batch: 1096, loss: 0.04648, acc: 0.98995\n",
      "global step 13700, epoch: 9, batch: 1196, loss: 0.03081, acc: 0.98990\n",
      "global step 13800, epoch: 9, batch: 1296, loss: 0.02649, acc: 0.98985\n",
      "global step 13900, epoch: 9, batch: 1396, loss: 0.10619, acc: 0.99002\n",
      "global step 14000, epoch: 9, batch: 1496, loss: 0.12895, acc: 0.99007\n",
      "eval loss: 0.76850, accu: 0.85090\n",
      "global step 14100, epoch: 10, batch: 33, loss: 0.00494, acc: 0.99242\n",
      "global step 14200, epoch: 10, batch: 133, loss: 0.00297, acc: 0.99436\n",
      "global step 14300, epoch: 10, batch: 233, loss: 0.00483, acc: 0.99430\n",
      "global step 14400, epoch: 10, batch: 333, loss: 0.00477, acc: 0.99324\n",
      "global step 14500, epoch: 10, batch: 433, loss: 0.00756, acc: 0.99307\n",
      "global step 14600, epoch: 10, batch: 533, loss: 0.01090, acc: 0.99285\n",
      "global step 14700, epoch: 10, batch: 633, loss: 0.00104, acc: 0.99294\n",
      "global step 14800, epoch: 10, batch: 733, loss: 0.00436, acc: 0.99297\n",
      "global step 14900, epoch: 10, batch: 833, loss: 0.02474, acc: 0.99289\n",
      "global step 15000, epoch: 10, batch: 933, loss: 0.00389, acc: 0.99295\n",
      "global step 15100, epoch: 10, batch: 1033, loss: 0.00418, acc: 0.99301\n",
      "global step 15200, epoch: 10, batch: 1133, loss: 0.00139, acc: 0.99283\n",
      "global step 15300, epoch: 10, batch: 1233, loss: 0.02979, acc: 0.99266\n",
      "global step 15400, epoch: 10, batch: 1333, loss: 0.00121, acc: 0.99278\n",
      "global step 15500, epoch: 10, batch: 1433, loss: 0.02626, acc: 0.99266\n",
      "global step 15600, epoch: 10, batch: 1533, loss: 0.01900, acc: 0.99272\n",
      "eval loss: 0.84223, accu: 0.84930\n",
      "global step 15700, epoch: 11, batch: 70, loss: 0.00296, acc: 0.99621\n",
      "global step 15800, epoch: 11, batch: 170, loss: 0.00299, acc: 0.99669\n",
      "global step 15900, epoch: 11, batch: 270, loss: 0.00025, acc: 0.99612\n",
      "global step 16000, epoch: 11, batch: 370, loss: 0.00096, acc: 0.99603\n",
      "global step 16100, epoch: 11, batch: 470, loss: 0.01594, acc: 0.99574\n",
      "global step 16200, epoch: 11, batch: 570, loss: 0.02193, acc: 0.99567\n",
      "global step 16300, epoch: 11, batch: 670, loss: 0.00094, acc: 0.99571\n",
      "global step 16400, epoch: 11, batch: 770, loss: 0.00258, acc: 0.99533\n",
      "global step 16500, epoch: 11, batch: 870, loss: 0.00431, acc: 0.99513\n",
      "global step 16600, epoch: 11, batch: 970, loss: 0.00250, acc: 0.99515\n",
      "global step 16700, epoch: 11, batch: 1070, loss: 0.02132, acc: 0.99515\n",
      "global step 16800, epoch: 11, batch: 1170, loss: 0.00451, acc: 0.99523\n",
      "global step 16900, epoch: 11, batch: 1270, loss: 0.05406, acc: 0.99525\n",
      "global step 17000, epoch: 11, batch: 1370, loss: 0.00108, acc: 0.99534\n",
      "global step 17100, epoch: 11, batch: 1470, loss: 0.06291, acc: 0.99534\n",
      "eval loss: 0.87533, accu: 0.85560\n",
      "global step 17200, epoch: 12, batch: 7, loss: 0.00711, acc: 0.99107\n",
      "global step 17300, epoch: 12, batch: 107, loss: 0.03804, acc: 0.99752\n",
      "global step 17400, epoch: 12, batch: 207, loss: 0.00116, acc: 0.99766\n",
      "global step 17500, epoch: 12, batch: 307, loss: 0.00404, acc: 0.99761\n",
      "global step 17600, epoch: 12, batch: 407, loss: 0.00044, acc: 0.99731\n",
      "global step 17700, epoch: 12, batch: 507, loss: 0.00191, acc: 0.99710\n",
      "global step 17800, epoch: 12, batch: 607, loss: 0.00234, acc: 0.99719\n",
      "global step 17900, epoch: 12, batch: 707, loss: 0.01276, acc: 0.99722\n",
      "global step 18000, epoch: 12, batch: 807, loss: 0.00263, acc: 0.99729\n",
      "global step 18100, epoch: 12, batch: 907, loss: 0.00079, acc: 0.99721\n",
      "global step 18200, epoch: 12, batch: 1007, loss: 0.00024, acc: 0.99725\n",
      "global step 18300, epoch: 12, batch: 1107, loss: 0.00010, acc: 0.99733\n",
      "global step 18400, epoch: 12, batch: 1207, loss: 0.02184, acc: 0.99735\n",
      "global step 18500, epoch: 12, batch: 1307, loss: 0.00064, acc: 0.99735\n",
      "global step 18600, epoch: 12, batch: 1407, loss: 0.00015, acc: 0.99736\n",
      "global step 18700, epoch: 12, batch: 1507, loss: 0.00053, acc: 0.99735\n",
      "eval loss: 0.95001, accu: 0.85510\n"
     ]
    }
   ],
   "source": [
    "import paddle.nn.functional as F\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = ernie_model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 100 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "    evaluate(ernie_model, criterion, metric, dev_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "ernie_model.save_pretrained('/home/aistudio/models/bq')\n",
    "ernie_tokenizer.save_pretrained('/home/aistudio/models/bq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 定义预测函数并进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义预测函数\n",
    "def predict(model, data, tokenizer, label_map, batch_size=1):\n",
    "    examples = []\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_words(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_len=128,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        # The last batch whose size is less than the config batch_size setting.\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "label_map = {0:'0', 1:'1'}\n",
    "results = predict(\n",
    "        ernie_model, test_, ernie_tokenizer, label_map, batch_size=batch_size)\n",
    "\n",
    "predict_results = []\n",
    "for idx, text in enumerate(test_):\n",
    "    predict_results.append({\"index\": idx, \"prediction\":results[idx]})\n",
    "predict_results = pd.DataFrame(predict_results)\n",
    "print(predict_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_results.to_csv('/home/aistudio/bq_corpus.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 学习心得\n",
    "\n",
    "paddlenlp提供了常用的预训练模型，对普通开发者友好，pytorch用户可以快速转换paddle。\n",
    "\n",
    "# 实践项目\n",
    "科大讯飞NLP算法赛baseline：学术论文分类挑战赛\n",
    "https://aistudio.baidu.com/aistudio/projectdetail/2201232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.2 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
