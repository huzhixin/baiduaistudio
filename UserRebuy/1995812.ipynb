{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 用户购买预测使用paddlerec的baseline\n",
    "$\\color{red}{Tips：该部分主要为介绍思路和paddlerec构建基线模型}$\n",
    "## 赛题介绍\n",
    "智能营销工具可以帮助商家预测用户购买的行为，本次比赛提供了一份品牌商家的历史订单数据，参赛选手需构建一个预测模型，预估用户人群在规定时间内产生购买行为的概率。\n",
    "该模型可应用于各种电商数据分析，以及百度电商开放平台， 不仅可以帮助商家基于平台流量，进行商品售卖、支付，还可以通过MarTech技术更精准地锁定核心用户，对用户的购买行为进行预测。\n",
    "\n",
    "- $\\color{red}{特别注意：数据进行了模拟生成，对某些特征含义进行了隐藏，并进行了脱敏处理}$\n",
    "\n",
    "- $\\color{red}{特别注意：本次比赛需要选手使用环境：飞桨PaddlePaddle>=1.7.2 , PaddleRec>=0.1以及以上版本参赛}$\n",
    "\n",
    "- $\\color{red}{特别注意：AI Studio上运行需要使用32G内存的高级版，本地运行同样需要配置较大的内存空间。}$\n",
    "\n",
    "[点击跳转至赛题页面](https://aistudio.baidu.com/aistudio/competition/detail/51)\n",
    "\n",
    "## 基线介绍\n",
    "### 运行方式\n",
    "\n",
    "本次基线基于飞桨PaddlePaddle1.8，PaddleRec v1.8.5版本，若本地运行则可能需要额外安装jupyter notebook环境、pandas模块等。  \n",
    "\n",
    "\n",
    "#### AI Studio (Notebook)运行\n",
    "\n",
    "依次运行下方的cell即可，若运行时修改了cell，推荐在右上角重启执行器后再以此运行，避免因内存未清空而产生报错。\n",
    "\n",
    "#### 本地运行\n",
    "\n",
    "fork本项目后点击右上角的“文件”——“导出Notebook为ipynb”，下载到本地后在`jupyter notebook`环境即可开始训练，生成的推理结果文件为`submission.csv`。\n",
    "\n",
    "### 设计思想\n",
    "\n",
    "#### 执行流程\n",
    "\n",
    "1. 配置预处理数据方案\n",
    "3. 开始训练\n",
    "4. 执行预测并产生结果文件\n",
    "\n",
    "#### 技术方案\n",
    "在本次赛题中，虽然赛题是一个二分类任务（用户购买、未购买），但从赛题数据看，属于比较典型的时间序列数据，也可以参照以往的线性回归任务的做法处理。 \n",
    "接下来将介绍技术方案中的一些细节问题以及method流程。\n",
    "\n",
    "##### label设计\n",
    "本次赛题反映了一个客观事实——在真实场景应用机器学习/深度学习技术时，通常是没有已经整理好的训练集、验证集、测试集，需要自己设计。\n",
    "\n",
    "比如赛题中提到，在比赛任务是预测下个月用户是否购买，下个月是哪个月？我们不妨设想自己是个业务经理，现在领导说做个模型，预测下个月你手上的客户是否会流失。所以在这类题目中，下个月就是提供的数据集截止日期之后的一个月。当然，如果比赛要求预测未来7天、未来15天的销售情况，道理也是一样的。\n",
    "\n",
    "在此类比赛的解决方案中，通常会有个时间滑窗的概念。比如按月进行时间滑窗，本题中数据到2013.8.31，默认提供的数据集划分设计如下（选手也可以自行设计数据集的划分）：\n",
    "- 训练集：选择某一天为截止时间，用截止时间前的3个月预测用户截止时间后的一个月是否购买；（保证截止时间后还存在一个月的数据）\n",
    "- 验证集：选择某一天为截止时间，用截止时间前的3个月预测用户截止时间后的一个月是否购买；（保证截止时间后还存在一个月的数据）\n",
    "- 测试集：用2013年6-8月的数据预测用户在9月是否购买（其实就是预测的目标）\n",
    "\n",
    "```python\n",
    "# 这是一个时间滑窗函数，获得dt之前minus天以来periods的dataframe，以便进一步计算\n",
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]\n",
    "```\n",
    "\n",
    "##### 时间滑窗特征构建\n",
    "> 注：更详细的时间滑窗特征工程的方法请参考[用户购买预测时间滑窗特征构建](https://aistudio.baidu.com/aistudio/projectdetail/276829)，本项目做了大幅缩减。\n",
    "\n",
    "时间滑窗在业务应用上被称为RFM模型，RFM模型最早是用来衡量客户价值和客户创利能力。理解RFM框架的思想是构造统计类特征的基础，其含义为：\n",
    "- R（Recency）：客户最近一次交易消费时间的间隔。R值越大，表示客户交易发生的日期越久，反之则表示客户交易发生的日期越近。\n",
    "- F（Frequency）：客户在最近一段时间内交易消费的次数。F值越大，表示客户交易越频繁，反之则表示客户交易不够活跃。\n",
    "- M（Monetary）：客户在最近一段时间内交易消费的金额。M值越大，表示客户价值越高，反之则表示客户价值越低。\n",
    "\n",
    "也就是说，时间滑窗特征本身是与业务紧密联系的，而在这类时间序列数据的比赛中，滑动时间窗口内的统计指标可以更加丰富，统计值一般会有最大值、最小值、均值、标准差、中位数、极差等。\n",
    "\n",
    "```python\n",
    "# 要计算统计指标特征的时间窗口\n",
    "for i in [14,30,60,91]:\n",
    "\ttmp = get_timespan(df_payment, t2018, i, i)\n",
    "   # 削去峰值的均值特征\n",
    "   X['mean_%s_decay' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "   # 中位数特征，在本赛题中基本不适用\n",
    "   # X['median_%s' % i] = tmp.median(axis=1).values\n",
    "   # 最小值特征，在本赛题中基本不适用\n",
    "   # X['min_%s' % i] = tmp_1.min(axis=1).values\n",
    "   # 最大值特征\n",
    "   X['max_%s' % i] = tmp.max(axis=1).values\n",
    "   # 标准差特征\n",
    "   # X['std_%s' % i] = tmp_1.std(axis=1).values\n",
    "   # 求和特征\n",
    "   X['sum_%s' % i] = tmp.sum(axis=1).values\n",
    "```\n",
    "##### 深度学习模型搭建\n",
    "参考[十分钟！全流程！从零搭建推荐系统](https://aistudio.baidu.com/aistudio/projectdetail/559336)和[乘风破浪的调参侠！玩转特征重要性～从此精通LR](https://aistudio.baidu.com/aistudio/projectdetail/618918)搭建三层深度神经网络。需要注意的是，由于神经网络对缺失值和稀疏数据敏感，对送入神经网络的特征需要做筛选。另外，选择哪种神经网络结构效果更好，需要参赛选手进一步探索。\n",
    "```python\n",
    "# 构建多层神经网络\n",
    "def net(self, input, is_infer=False):\n",
    "        self.data_inputs = input[0]\n",
    "        self.label_input = input[1]\n",
    "\n",
    "        fc1 = fluid.layers.fc(input = self.data_inputs,size = self.fc1_size ,act='relu')\n",
    "        fc2 = fluid.layers.fc(input = fc1 ,size = self.fc2_size ,act='relu')\n",
    "        predict = fluid.layers.fc(input = fc2 ,size=2,act='softmax')\n",
    "        self.predict = predict\n",
    "\n",
    "        auc, batch_auc, _ = fluid.layers.auc(input=self.predict,\n",
    "                                             label=self.label_input,\n",
    "                                             num_thresholds=2**12,\n",
    "                                             slide_steps=20)\n",
    "        if is_infer:\n",
    "            self._infer_results[\"AUC\"] = auc\n",
    "            self._infer_results[\"BATCH_AUC\"] = batch_auc\n",
    "            return\n",
    "\n",
    "        self._metrics[\"AUC\"] = auc\n",
    "        self._metrics[\"BATCH_AUC\"] = batch_auc\n",
    "        cost = fluid.layers.cross_entropy(\n",
    "            input=self.predict, label=self.label_input)\n",
    "        avg_cost = fluid.layers.reduce_mean(cost)\n",
    "        self._cost = avg_cost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据预处理 - 数据集划分与特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np  #\n",
    "import pandas as pd  #\n",
    "from datetime import datetime, date, timedelta\n",
    "from scipy.stats import skew  # for some statistics\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "# from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV,Ridge,Lasso,ElasticNet\n",
    "# from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor,RandomForestClassifier\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# from sklearn.svm import SVR, LinearSVC\n",
    "# from sklearn.pipeline import make_pipeline,Pipeline\n",
    "# from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler,MinMaxScaler\n",
    "# from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from itertools import product\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import gc\n",
    "from datetime import date, timedelta\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "import paddle.fluid.dygraph as dygraph\n",
    "from paddle.fluid.dygraph import Linear\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PATH = './data/data19383/'\n",
    "train = pd.read_csv(PATH + 'train.csv')\n",
    "# train = pd.read_csv('./data/data19383/train.csv', usecols=[2, 3, 4, 6, 7, 18])\n",
    "# set index to ID to avoid droping it later\n",
    "# 把测试集的id列作为索引，防止误删\n",
    "test  = pd.read_csv(PATH + 'submission.csv').set_index('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_detail_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_amount</th>\n",
       "      <th>order_pay_time</th>\n",
       "      <th>is_customer_rate</th>\n",
       "      <th>order_detail_goods_num</th>\n",
       "      <th>order_detail_amount</th>\n",
       "      <th>order_detail_discount</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>goods_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2306861</th>\n",
       "      <td>3685490</td>\n",
       "      <td>3238357</td>\n",
       "      <td>707.7</td>\n",
       "      <td>2013-01-24 00:24:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>179.8</td>\n",
       "      <td>298.2</td>\n",
       "      <td>2826572</td>\n",
       "      <td>1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306862</th>\n",
       "      <td>3685491</td>\n",
       "      <td>3238356</td>\n",
       "      <td>775.9</td>\n",
       "      <td>2012-11-11 17:35:05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126.9</td>\n",
       "      <td>172.1</td>\n",
       "      <td>2826572</td>\n",
       "      <td>2103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306863</th>\n",
       "      <td>3685492</td>\n",
       "      <td>3238357</td>\n",
       "      <td>707.7</td>\n",
       "      <td>2013-01-24 00:24:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>2826572</td>\n",
       "      <td>3153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306864</th>\n",
       "      <td>3685493</td>\n",
       "      <td>3238356</td>\n",
       "      <td>775.9</td>\n",
       "      <td>2012-11-11 17:35:05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74.9</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2826572</td>\n",
       "      <td>1778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306865</th>\n",
       "      <td>3685494</td>\n",
       "      <td>3238357</td>\n",
       "      <td>707.7</td>\n",
       "      <td>2013-01-24 00:24:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94.9</td>\n",
       "      <td>104.9</td>\n",
       "      <td>2826572</td>\n",
       "      <td>2128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306866</th>\n",
       "      <td>3685495</td>\n",
       "      <td>3238358</td>\n",
       "      <td>199.0</td>\n",
       "      <td>2013-01-10 19:24:31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.9</td>\n",
       "      <td>139.1</td>\n",
       "      <td>2826573</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306867</th>\n",
       "      <td>3685496</td>\n",
       "      <td>3238359</td>\n",
       "      <td>299.8</td>\n",
       "      <td>2013-01-27 15:00:27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.9</td>\n",
       "      <td>2826574</td>\n",
       "      <td>2513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306868</th>\n",
       "      <td>3685497</td>\n",
       "      <td>3238359</td>\n",
       "      <td>299.8</td>\n",
       "      <td>2013-01-27 15:00:27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>89.9</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2826574</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306869</th>\n",
       "      <td>3685498</td>\n",
       "      <td>3238360</td>\n",
       "      <td>168.0</td>\n",
       "      <td>2012-11-11 00:10:37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.9</td>\n",
       "      <td>91.1</td>\n",
       "      <td>2826574</td>\n",
       "      <td>1423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306870</th>\n",
       "      <td>3685499</td>\n",
       "      <td>3238361</td>\n",
       "      <td>102.0</td>\n",
       "      <td>2013-07-10 14:22:14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.9</td>\n",
       "      <td>52.1</td>\n",
       "      <td>2826574</td>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         order_detail_id  order_id  order_amount       order_pay_time  \\\n",
       "2306861          3685490   3238357         707.7  2013-01-24 00:24:40   \n",
       "2306862          3685491   3238356         775.9  2012-11-11 17:35:05   \n",
       "2306863          3685492   3238357         707.7  2013-01-24 00:24:40   \n",
       "2306864          3685493   3238356         775.9  2012-11-11 17:35:05   \n",
       "2306865          3685494   3238357         707.7  2013-01-24 00:24:40   \n",
       "2306866          3685495   3238358         199.0  2013-01-10 19:24:31   \n",
       "2306867          3685496   3238359         299.8  2013-01-27 15:00:27   \n",
       "2306868          3685497   3238359         299.8  2013-01-27 15:00:27   \n",
       "2306869          3685498   3238360         168.0  2012-11-11 00:10:37   \n",
       "2306870          3685499   3238361         102.0  2013-07-10 14:22:14   \n",
       "\n",
       "         is_customer_rate  order_detail_goods_num  order_detail_amount  \\\n",
       "2306861               0.0                     2.0                179.8   \n",
       "2306862               0.0                     1.0                126.9   \n",
       "2306863               0.0                     1.0                  0.0   \n",
       "2306864               0.0                     1.0                 74.9   \n",
       "2306865               0.0                     1.0                 94.9   \n",
       "2306866               0.0                     1.0                 59.9   \n",
       "2306867               0.0                     1.0                  0.0   \n",
       "2306868               0.0                     1.0                 89.9   \n",
       "2306869               0.0                     1.0                 76.9   \n",
       "2306870               0.0                     1.0                 49.9   \n",
       "\n",
       "         order_detail_discount  customer_id  goods_id  \n",
       "2306861                  298.2      2826572      1478  \n",
       "2306862                  172.1      2826572      2103  \n",
       "2306863                   29.9      2826572      3153  \n",
       "2306864                  105.0      2826572      1778  \n",
       "2306865                  104.9      2826572      2128  \n",
       "2306866                  139.1      2826573      1173  \n",
       "2306867                   59.9      2826574      2513  \n",
       "2306868                  150.0      2826574       998  \n",
       "2306869                   91.1      2826574      1423  \n",
       "2306870                   52.1      2826574      1043  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集概况\r\n",
    "train[['order_detail_id','order_id','order_amount','order_pay_time','is_customer_rate','order_detail_goods_num','order_detail_amount','order_detail_discount','customer_id','goods_id']].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1585986"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174770"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['customer_id'][train.order_pay_time>'2013-07-31'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2078390"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train.order_pay_time<'2013-07-31'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\r\n",
    "y = range(1,2)\r\n",
    " \r\n",
    "plt.bar(['not buy','buy'], [1585986-174770,174770], alpha=0.5, width=0.3, color='lightblue', edgecolor='grey', lw=3)\r\n",
    "plt.title('Sales in August 2013', fontsize=10)\r\n",
    "for a, b in zip(['not buy','buy'], [1585986-174770,174770]):\r\n",
    "    plt.text(a, b + 0.05, '%.0f' % b, ha='center', va='bottom', fontsize=10)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集使用内容 510.40096282958984 MB\n",
      "测试集使用内存 24.200225830078125 MB\n"
     ]
    }
   ],
   "source": [
    "# 对于特别大的文件，我们需要做一些内存检查\n",
    "mem_train = train.memory_usage(index=True).sum()\n",
    "mem_test=test.memory_usage(index=True).sum()\n",
    "print(u\"训练集使用内容 \"+ str(mem_train/ 1024**2)+\" MB\")\n",
    "print(u\"测试集使用内存 \"+ str(mem_test/ 1024**2)+\" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 内存优化脚本\n",
    "- 参考[缓解pandas中DataFrame占用内存过大的问题](https://blog.csdn.net/wj1066/article/details/81124959)\n",
    "- 效果非常显著，有效避免内存溢出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# @from: https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65/code\r\n",
    "# @liscense: Apache 2.0\r\n",
    "# @author: weijian\r\n",
    "def reduce_mem_usage(props):\r\n",
    "    # 计算当前内存\r\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024 ** 2\r\n",
    "    print(\"Memory usage of the dataframe is :\", start_mem_usg, \"MB\")\r\n",
    "    \r\n",
    "    # 哪些列包含空值，空值用-999填充。why：因为np.nan当做float处理\r\n",
    "    NAlist = []\r\n",
    "    for col in props.columns:\r\n",
    "        # 这里只过滤了objectd格式，如果你的代码中还包含其他类型，请一并过滤\r\n",
    "        if (props[col].dtypes != object):\r\n",
    "            \r\n",
    "            # print(\"**************************\")\r\n",
    "            # print(\"columns: \", col)\r\n",
    "            # print(\"dtype before\", props[col].dtype)\r\n",
    "            \r\n",
    "            # 判断是否是int类型\r\n",
    "            isInt = False\r\n",
    "            mmax = props[col].max()\r\n",
    "            mmin = props[col].min()\r\n",
    "            \r\n",
    "            # Integer does not support NA, therefore Na needs to be filled\r\n",
    "            if not np.isfinite(props[col]).all():\r\n",
    "                NAlist.append(col)\r\n",
    "                props[col].fillna(-999, inplace=True) # 用-999填充\r\n",
    "                \r\n",
    "            # test if column can be converted to an integer\r\n",
    "            asint = props[col].fillna(0).astype(np.int64)\r\n",
    "            result = np.fabs(props[col] - asint)\r\n",
    "            result = result.sum()\r\n",
    "            if result < 0.01: # 绝对误差和小于0.01认为可以转换的，要根据task修改\r\n",
    "                isInt = True\r\n",
    "            \r\n",
    "            # make interger / unsigned Integer datatypes\r\n",
    "            if isInt:\r\n",
    "                if mmin >= 0: # 最小值大于0，转换成无符号整型\r\n",
    "                    if mmax <= 255:\r\n",
    "                        props[col] = props[col].astype(np.uint8)\r\n",
    "                    elif mmax <= 65535:\r\n",
    "                        props[col] = props[col].astype(np.uint16)\r\n",
    "                    elif mmax <= 4294967295:\r\n",
    "                        props[col] = props[col].astype(np.uint32)\r\n",
    "                    else:\r\n",
    "                        props[col] = props[col].astype(np.uint64)\r\n",
    "                else: # 转换成有符号整型\r\n",
    "                    if mmin > np.iinfo(np.int8).min and mmax < np.iinfo(np.int8).max:\r\n",
    "                        props[col] = props[col].astype(np.int8)\r\n",
    "                    elif mmin > np.iinfo(np.int16).min and mmax < np.iinfo(np.int16).max:\r\n",
    "                        props[col] = props[col].astype(np.int16)\r\n",
    "                    elif mmin > np.iinfo(np.int32).min and mmax < np.iinfo(np.int32).max:\r\n",
    "                        props[col] = props[col].astype(np.int32)\r\n",
    "                    elif mmin > np.iinfo(np.int64).min and mmax < np.iinfo(np.int64).max:\r\n",
    "                        props[col] = props[col].astype(np.int64)  \r\n",
    "            else: # 注意：这里对于float都转换成float16，需要根据你的情况自己更改\r\n",
    "                props[col] = props[col].astype(np.float16)\r\n",
    "            \r\n",
    "            # print(\"dtype after\", props[col].dtype)\r\n",
    "            # print(\"********************************\")\r\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\r\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \r\n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\r\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\r\n",
    "    return props, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集使用内容 349.8006982803345 MB\n",
      "测试集使用内存 24.200225830078125 MB\n"
     ]
    }
   ],
   "source": [
    "# 处理id字段\n",
    "train['order_detail_id'] = train['order_detail_id'].astype(np.uint32)\n",
    "train['order_id'] = train['order_id'].astype(np.uint32)\n",
    "train['customer_id'] = train['customer_id'].astype(np.uint32)\n",
    "train['goods_id'] = train['goods_id'].astype(np.uint32)\n",
    "train['goods_class_id'] = train['goods_class_id'].astype(np.uint32)\n",
    "train['member_id'] = train['member_id'].astype(np.uint32)\n",
    "# 处理状态字段，这里同时处理空值，将空值置为0\n",
    "train['order_status'] = train['order_status'].astype(np.uint8)\n",
    "train['goods_has_discount'] = train['goods_has_discount'].astype(np.uint8)\n",
    "train[\"is_member_actived\"].fillna(0, inplace=True)\n",
    "train[\"is_member_actived\"]=train[\"is_member_actived\"].astype(np.int8)\n",
    "train[\"member_status\"].fillna(0, inplace=True)\n",
    "train[\"member_status\"]=train[\"member_status\"].astype(np.int8)\n",
    "train[\"customer_gender\"].fillna(0, inplace=True)\n",
    "train[\"customer_gender\"]=train[\"customer_gender\"].astype(np.int8)\n",
    "train['is_customer_rate'] = train['is_customer_rate'].astype(np.uint8)\n",
    "train['order_detail_status'] = train['order_detail_status'].astype(np.uint8)\n",
    "# 处理日期\n",
    "train['goods_list_time']=pd.to_datetime(train['goods_list_time'],format=\"%Y-%m-%d\")\n",
    "train['order_pay_time']=pd.to_datetime(train['order_pay_time'],format=\"%Y-%m-%d\")\n",
    "train['goods_delist_time']=pd.to_datetime(train['goods_delist_time'],format=\"%Y-%m-%d\")\n",
    "# 检查内存使用\n",
    "mem_train = train.memory_usage(index=True).sum()\n",
    "mem_test=test.memory_usage(index=True).sum()\n",
    "print(u\"训练集使用内容 \"+ str(mem_train/ 1024**2)+\" MB\")\n",
    "print(u\"测试集使用内存 \"+ str(mem_test/ 1024**2)+\" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 构造时间滑窗特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 每日付款金额"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# 将用户下单金额按天进行汇总\r\n",
    "# df = train[train.order_status<101][train.order_pay_time>'2013-02-01']\r\n",
    "df = train[train.order_pay_time>'2013-02-01']\r\n",
    "df['date'] = pd.DatetimeIndex(df['order_pay_time']).date\r\n",
    "df_payment = df[['customer_id','date','order_total_payment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685471"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_payment['customer_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "注意，成功交易的客户数量不等于全部客户数量，说明有相当一部分客户虽然下过单，但是没有成功的订单，那么这些客户自然应当算在训练集之外。\n",
    "数据合并时，由于`test.csv`中，已经设置了默认0值，只需要和训练后的预测标签做一个`left join`就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_payment = df_payment.groupby(['date','customer_id']).agg({'order_total_payment': ['sum']})\n",
    "df_payment.columns = ['day_total_payment']\n",
    "df_payment.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_payment = df_payment.set_index(\n",
    "    [\"customer_id\", \"date\"])[[\"day_total_payment\"]].unstack(level=-1).fillna(0)\n",
    "df_payment.columns = df_payment.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 每日购买商品数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_goods = df[['customer_id','date','order_total_num']]\r\n",
    "df_goods = df_goods.groupby(['date','customer_id']).agg({'order_total_num': ['sum']})\r\n",
    "df_goods.columns = ['day_total_num']\r\n",
    "df_goods.reset_index(inplace=True)\r\n",
    "df_goods = df_goods.set_index(\r\n",
    "    [\"customer_id\", \"date\"])[[\"day_total_num\"]].unstack(level=-1).fillna(0)\r\n",
    "df_goods.columns = df_goods.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "该场景每天都有成交记录，这样就不需要考虑生成完整时间段填充的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这是一个时间滑窗函数，获得dt之前minus天以来periods的dataframe，以便进一步计算\n",
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. 构造dataset这里有个取巧的地方，因为要预测的9月份除了开学季以外不是非常特殊的月份，因此主要考虑近期的因素，数据集的开始时间也是2月1日，尽量避免了双十一、元旦假期的影响，当然春节假期继续保留。同时，构造数据集的时候保留了customer_id，主要为了与其它特征做整合。\n",
    "2. 通过一个函数整合付款金额和商品数量的时间滑窗，主要是因为分开做到时候合并占用内存更大，并且函数最后在返回值处做了内存优化，用时间代价尽可能避免内存溢出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(df_payment, df_goods, t2018, is_train=True):\r\n",
    "    X = {}\r\n",
    "    # 整合用户id\r\n",
    "    tmp = df_payment.reset_index()\r\n",
    "    X['customer_id'] = tmp['customer_id']\r\n",
    "    # 消费特征\r\n",
    "    print('Preparing payment feature...')\r\n",
    "    for i in [14,30,60,91]:\r\n",
    "        tmp = get_timespan(df_payment, t2018, i, i)\r\n",
    "        # X['diff_%s_mean' % i] = tmp_1.diff(axis=1).mean(axis=1).values\r\n",
    "        X['mean_%s_decay' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\r\n",
    "        # X['mean_%s' % i] = tmp_1.mean(axis=1).values\r\n",
    "        # X['median_%s' % i] = tmp.median(axis=1).values\r\n",
    "        # X['min_%s' % i] = tmp_1.min(axis=1).values\r\n",
    "        X['max_%s' % i] = tmp.max(axis=1).values\r\n",
    "        # X['std_%s' % i] = tmp_1.std(axis=1).values\r\n",
    "        X['sum_%s' % i] = tmp.sum(axis=1).values\r\n",
    "    for i in [14,30,60,91]:\r\n",
    "        tmp = get_timespan(df_payment, t2018 + timedelta(days=-7), i, i)\r\n",
    "        X['mean_%s_decay_2' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\r\n",
    "        # X['mean_%s_2' % i] = tmp_2.mean(axis=1).values\r\n",
    "        # X['median_%s_2' % i] = tmp.median(axis=1).values\r\n",
    "        # X['min_%s_2' % i] = tmp_2.min(axis=1).values\r\n",
    "        X['max_%s_2' % i] = tmp.max(axis=1).values\r\n",
    "        # X['std_%s_2' % i] = tmp_2.std(axis=1).values\r\n",
    "    for i in [14,30,60,91]:\r\n",
    "        tmp = get_timespan(df_payment, t2018, i, i)\r\n",
    "        X['has_sales_days_in_last_%s' % i] = (tmp != 0).sum(axis=1).values\r\n",
    "        X['last_has_sales_day_in_last_%s' % i] = i - ((tmp != 0) * np.arange(i)).max(axis=1).values\r\n",
    "        X['first_has_sales_day_in_last_%s' % i] = ((tmp != 0) * np.arange(i, 0, -1)).max(axis=1).values\r\n",
    "\r\n",
    "    # 对此处进行微调，主要考虑近期因素\r\n",
    "    for i in range(1, 4):\r\n",
    "        X['day_%s_2018' % i] = get_timespan(df_payment, t2018, i*30, 30).sum(axis=1).values\r\n",
    "    # 商品数量特征，这里故意把时间和消费特征错开，提高时间滑窗的覆盖面\r\n",
    "    print('Preparing num feature...')\r\n",
    "    for i in [21,49,84]:\r\n",
    "            tmp = get_timespan(df_goods, t2018, i, i)\r\n",
    "            # X['goods_diff_%s_mean' % i] = tmp_1.diff(axis=1).mean(axis=1).values\r\n",
    "            # X['goods_mean_%s_decay' % i] = (tmp_1 * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\r\n",
    "            X['goods_mean_%s' % i] = tmp.mean(axis=1).values\r\n",
    "            # X['goods_median_%s' % i] = tmp.median(axis=1).values\r\n",
    "            # X['goods_min_%s' % i] = tmp_1.min(axis=1).values\r\n",
    "            X['goods_max_%s' % i] = tmp.max(axis=1).values\r\n",
    "            # X['goods_std_%s' % i] = tmp_1.std(axis=1).values\r\n",
    "            X['goods_sum_%s' % i] = tmp.sum(axis=1).values\r\n",
    "    for i in [21,49,84]:    \r\n",
    "            tmp = get_timespan(df_goods, t2018 + timedelta(weeks=-1), i, i)\r\n",
    "            # X['goods_diff_%s_mean_2' % i] = tmp_2.diff(axis=1).mean(axis=1).values\r\n",
    "            # X['goods_mean_%s_decay_2' % i] = (tmp_2 * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\r\n",
    "            X['goods_mean_%s_2' % i] = tmp.mean(axis=1).values\r\n",
    "            # X['goods_median_%s_2' % i] = tmp.median(axis=1).values\r\n",
    "            # X['goods_min_%s_2' % i] = tmp_2.min(axis=1).values\r\n",
    "            X['goods_max_%s_2' % i] = tmp.max(axis=1).values\r\n",
    "            X['goods_sum_%s_2' % i] = tmp.sum(axis=1).values\r\n",
    "    for i in [21,49,84]:    \r\n",
    "            tmp = get_timespan(df_goods, t2018, i, i)\r\n",
    "            X['goods_has_sales_days_in_last_%s' % i] = (tmp > 0).sum(axis=1).values\r\n",
    "            X['goods_last_has_sales_day_in_last_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\r\n",
    "            X['goods_first_has_sales_day_in_last_%s' % i] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\r\n",
    "\r\n",
    "\r\n",
    "    # 对此处进行微调，主要考虑近期因素\r\n",
    "    for i in range(1, 4):\r\n",
    "        X['goods_day_%s_2018' % i] = get_timespan(df_goods, t2018, i*28, 28).sum(axis=1).values\r\n",
    "\r\n",
    "    X = pd.DataFrame(X)\r\n",
    "    \r\n",
    "    reduce_mem_usage(X)\r\n",
    "    \r\n",
    "    if is_train:\r\n",
    "        # 这样转换之后，打标签直接用numpy切片就可以了\r\n",
    "        # 当然这里前提是确认付款总额没有负数的问题\r\n",
    "        X['label'] = df_goods[pd.date_range(t2018, periods=30)].max(axis=1).values\r\n",
    "        X['label'][X['label'] > 0] = 1\r\n",
    "        return X\r\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "num_days = 4\r\n",
    "t2017 = date(2013, 7, 1)\r\n",
    "X_l, y_l = [], []\r\n",
    "for i in range(num_days):\r\n",
    "    delta = timedelta(days=7 * i)\r\n",
    "    # X_tmp, y_tmp = prepare_dataset(df_payment, df_goods, t2017 + delta)\r\n",
    "    X_tmp = prepare_dataset(df_payment, df_goods, t2017 + delta)\r\n",
    "    X_tmp = pd.concat([X_tmp], axis=1)\r\n",
    "\r\n",
    "    X_l.append(X_tmp)\r\n",
    "    # y_l.append(y_tmp)\r\n",
    "\r\n",
    "X_train = pd.concat(X_l, axis=0)\r\n",
    "# y_train = np.concatenate(y_l, axis=0)\r\n",
    "\r\n",
    "del X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "X_test = prepare_dataset(df_payment, df_goods, date(2013, 9, 1), is_train=False)\r\n",
    "X_test = pd.concat([X_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 中间结果保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test.to_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 选取参与训练的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载之前特征工程的结果\r\n",
    "X_train = pd.read_csv('X_train.csv')\r\n",
    "X_test = pd.read_csv('X_test.csv')\r\n",
    "customer_id = X_test[\"customer_id\"].values.tolist()\r\n",
    "X_train.drop(['Unnamed: 0','customer_id'], inplace=True, axis=1)\r\n",
    "X_test.drop(['Unnamed: 0','customer_id'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_14_decay</th>\n",
       "      <th>max_14</th>\n",
       "      <th>sum_14</th>\n",
       "      <th>mean_30_decay</th>\n",
       "      <th>max_30</th>\n",
       "      <th>sum_30</th>\n",
       "      <th>mean_60_decay</th>\n",
       "      <th>max_60</th>\n",
       "      <th>sum_60</th>\n",
       "      <th>mean_91_decay</th>\n",
       "      <th>...</th>\n",
       "      <th>goods_has_sales_days_in_last_49</th>\n",
       "      <th>goods_last_has_sales_day_in_last_49</th>\n",
       "      <th>goods_first_has_sales_day_in_last_49</th>\n",
       "      <th>goods_has_sales_days_in_last_84</th>\n",
       "      <th>goods_last_has_sales_day_in_last_84</th>\n",
       "      <th>goods_first_has_sales_day_in_last_84</th>\n",
       "      <th>goods_day_1_2018</th>\n",
       "      <th>goods_day_2_2018</th>\n",
       "      <th>goods_day_3_2018</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.537</td>\n",
       "      <td>39.9</td>\n",
       "      <td>39.9</td>\n",
       "      <td>3.537</td>\n",
       "      <td>39.9</td>\n",
       "      <td>39.9</td>\n",
       "      <td>3.537</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.530</td>\n",
       "      <td>98.9</td>\n",
       "      <td>197.8</td>\n",
       "      <td>2.530</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_14_decay  max_14  sum_14  mean_30_decay  max_30  sum_30  \\\n",
       "0            0.0     0.0     0.0          0.000     0.0     0.0   \n",
       "1            0.0     0.0     0.0          0.000     0.0     0.0   \n",
       "2            0.0     0.0     0.0          3.537    39.9    39.9   \n",
       "3            0.0     0.0     0.0          0.000     0.0     0.0   \n",
       "4            0.0     0.0     0.0          0.000     0.0     0.0   \n",
       "\n",
       "   mean_60_decay  max_60  sum_60  mean_91_decay  ...  \\\n",
       "0          0.000     0.0     0.0          0.000  ...   \n",
       "1          0.000     0.0     0.0          0.000  ...   \n",
       "2          3.537    39.9    39.9          3.537  ...   \n",
       "3          0.000     0.0     0.0          0.000  ...   \n",
       "4          2.530    98.9   197.8          2.530  ...   \n",
       "\n",
       "   goods_has_sales_days_in_last_49  goods_last_has_sales_day_in_last_49  \\\n",
       "0                                0                                   49   \n",
       "1                                0                                   49   \n",
       "2                                1                                   24   \n",
       "3                                0                                   49   \n",
       "4                                1                                   37   \n",
       "\n",
       "   goods_first_has_sales_day_in_last_49  goods_has_sales_days_in_last_84  \\\n",
       "0                                     0                                0   \n",
       "1                                     0                                0   \n",
       "2                                    24                                1   \n",
       "3                                     0                                0   \n",
       "4                                    37                                2   \n",
       "\n",
       "   goods_last_has_sales_day_in_last_84  goods_first_has_sales_day_in_last_84  \\\n",
       "0                                   84                                     0   \n",
       "1                                   84                                     0   \n",
       "2                                   24                                    24   \n",
       "3                                   84                                     0   \n",
       "4                                   37                                    56   \n",
       "\n",
       "   goods_day_1_2018  goods_day_2_2018  goods_day_3_2018  label  \n",
       "0                 0                 0                 0    0.0  \n",
       "1                 0                 0                 0    0.0  \n",
       "2                 1                 0                 0    0.0  \n",
       "3                 0                 0                 0    1.0  \n",
       "4                 0                 2                 0    1.0  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mean_14_decay', 'max_14', 'sum_14', 'mean_30_decay', 'max_30',\n",
      "       'sum_30', 'mean_60_decay', 'max_60', 'sum_60', 'mean_91_decay',\n",
      "       'max_91', 'sum_91', 'mean_14_decay_2', 'max_14_2', 'mean_30_decay_2',\n",
      "       'max_30_2', 'mean_60_decay_2', 'max_60_2', 'mean_91_decay_2',\n",
      "       'max_91_2', 'has_sales_days_in_last_14',\n",
      "       'last_has_sales_day_in_last_14', 'first_has_sales_day_in_last_14',\n",
      "       'has_sales_days_in_last_30', 'last_has_sales_day_in_last_30',\n",
      "       'first_has_sales_day_in_last_30', 'has_sales_days_in_last_60',\n",
      "       'last_has_sales_day_in_last_60', 'first_has_sales_day_in_last_60',\n",
      "       'has_sales_days_in_last_91', 'last_has_sales_day_in_last_91',\n",
      "       'first_has_sales_day_in_last_91', 'day_1_2018', 'day_2_2018',\n",
      "       'day_3_2018', 'goods_mean_21', 'goods_max_21', 'goods_sum_21',\n",
      "       'goods_mean_49', 'goods_max_49', 'goods_sum_49', 'goods_mean_84',\n",
      "       'goods_max_84', 'goods_sum_84', 'goods_mean_21_2', 'goods_max_21_2',\n",
      "       'goods_sum_21_2', 'goods_mean_49_2', 'goods_max_49_2', 'goods_sum_49_2',\n",
      "       'goods_mean_84_2', 'goods_max_84_2', 'goods_sum_84_2',\n",
      "       'goods_has_sales_days_in_last_21',\n",
      "       'goods_last_has_sales_day_in_last_21',\n",
      "       'goods_first_has_sales_day_in_last_21',\n",
      "       'goods_has_sales_days_in_last_49',\n",
      "       'goods_last_has_sales_day_in_last_49',\n",
      "       'goods_first_has_sales_day_in_last_49',\n",
      "       'goods_has_sales_days_in_last_84',\n",
      "       'goods_last_has_sales_day_in_last_84',\n",
      "       'goods_first_has_sales_day_in_last_84', 'goods_day_1_2018',\n",
      "       'goods_day_2_2018', 'goods_day_3_2018', 'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label = X_train[\"label\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = (X_train - X_train.min()) / (X_train.max() - X_train.min())\r\n",
    "X_test = (X_test - X_test.min()) / (X_test.max() - X_test.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[\"label\"] = label\r\n",
    "X_test[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2741884 2193507 548377\n"
     ]
    }
   ],
   "source": [
    "# 数据分割，在全量的训练集中划分出训练集和验证集\n",
    "size = X_train.shape[0]\n",
    "X_valid = X_train.iloc[int(size*0.8):]\n",
    "X_train = X_train.iloc[:int(size*0.8)]\n",
    "print(size,X_train.shape[0],X_valid.shape[0])\n",
    "\n",
    "customer_id = pd.DataFrame(columns=['customer_id'],data=customer_id)\n",
    "customer_id.to_csv(\"test_customer_id.csv\",index=False,header=False)\n",
    "\n",
    "X_valid.to_csv(\"valid_1.csv\",index=False,header=False)\n",
    "X_train.to_csv(\"train_1.csv\",index=False,header=False)\n",
    "X_test.to_csv(\"test_1.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 使用PaddleRec构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 基本文件结构\n",
    "在刚接触PaddleRec的时候，您需要了解其中每个模型最基础的组成部分。\n",
    "\n",
    "#### 1. data  \n",
    "PaddleRec的模型下面通常都会配有相应的样例数据，供使用者一键启动快速体验。而数据通常放在模型相应目录下的data目录中。有些模型中还会在data目录下更详细的分为训练数据目录和测试数据目录。同时一些下载数据和数据预处理的脚本通常也会放在这个目录下。  \n",
    "\n",
    "#### 2. model.py  \n",
    "model.py为模型文件，在其中定义模型的组网。如果您希望对模型进行改动或者添加自定义的模型，可以点击[这里](https://github.com/PaddlePaddle/PaddleRec/blob/master/doc/model_develop.md)查看更加详细的教程  \n",
    "\n",
    "#### 3. config.yaml  \n",
    "config.yaml中存放着模型的各种配置。其中又大体分为几个模块：\n",
    "\n",
    "\n",
    "- workspace  指定model/reader/data所在位置\n",
    "\n",
    "- dataset  指定数据输入的具体方式\n",
    "\n",
    "- hyper_parameters  模型中需要用到的超参数\n",
    "\n",
    "- mode  指定当次运行使用哪些runner\n",
    "\n",
    "- runner&phase  指定运行的具体方式和参数\n",
    "\n",
    "\n",
    "更加具体的参数请点击[这里](https://github.com/PaddlePaddle/PaddleRec/blob/master/doc/yaml.md)查看更加详细的教程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 构建Reader\n",
    "在PaddleRec中，我们有两种数据输入的方式。您可以选择直接使用PaddleRec内置的Reader，或者为您的模型自定义Reader。 \n",
    "#### 使用PaddleRec内置的Reader   \n",
    "\n",
    "当您的数据集格式为slot:feasign这种模式，或者可以预处理为这种格式时，可以直接使用PaddleRec内置的Reader\n",
    "```\n",
    "Slot : Feasign 是什么？\n",
    "Slot直译是槽位，在推荐工程中，是指某一个宽泛的特征类别，比如用户ID、性别、年龄就是Slot，Feasign则是具体值，比如：12345，男，20岁。\n",
    "在实践过程中，很多特征槽位不是单一属性，或无法量化并且离散稀疏的，比如某用户兴趣爱好有三个：游戏/足球/数码，且每个具体兴趣又有多个特征维度，则在兴趣爱好这个Slot兴趣槽位中，就会有多个Feasign值。\n",
    "PaddleRec在读取数据时，每个Slot ID对应的特征，支持稀疏，且支持变长，可以非常灵活的支持各种场景的推荐模型训练。\n",
    "```\n",
    "将数据集处理为slot:feasign这种模式的数据后，在相应的配置文件config.yaml中填写以空格分开的sparse_slots表示稀疏特征的列表，以空格分开dense_slots表示稠密特征的列表，模型即可从数据集中按slot列表读取相应的特征。  \n",
    "例如本教程中的yaml配置：\n",
    "```\n",
    "#读取从logid到label的10个稀疏特征特征\n",
    "sparse_slots: \"logid time userid gender age occupation movieid title genres label\"\n",
    "dense_slots: \"\"\n",
    "```\n",
    "配置好了之后，这些slot对应的variable在model中可以使用如下方式调用：\n",
    "```\n",
    "self._sparse_data_var\n",
    "self._dense_data_var\n",
    "```\n",
    "若要详细了解这种输入方式，点击[这里](https://github.com/PaddlePaddle/PaddleRec/blob/master/doc/slot_reader.md)了解更多\n",
    "#### 使用自定义Reader\n",
    "当您的数据集格式并不方便处理为slot:feasign这种模式，PaddleRec也支持您使用自定义的格式进行输入。不过您需要一个单独的python文件进行描述。  \n",
    "实现自定义的reader具体流程如下：  \n",
    "1. 首先我们需要引入Reader基类\n",
    "    ```python\n",
    "    from paddlerec.core.reader import ReaderBase\n",
    "    ```\n",
    "2. 创建一个子类，继承Reader的基类。\n",
    "    ```python\n",
    "    class Reader(ReaderBase):\n",
    "        def init(self):\n",
    "            pass\n",
    "\n",
    "        def generator_sample(self, line):\n",
    "            pass\n",
    "    ```\n",
    "3. 在`init(self)`函数中声明一些在数据读取中会用到的变量，必要时可以在`config.yaml`文件中配置变量，利用`env.get_global_env()`拿到。\n",
    "4. 继承并实现基类中的`generate_sample(self, line)`函数，逐行读取数据。\n",
    "   - 该函数应返回一个可以迭代的reader方法(带有yield的函数不再是一个普通的函数，而是一个生成器generator，成为了可以迭代的对象，等价于一个数组、链表、文件、字符串etc.)\n",
    "   - 在这个可以迭代的函数中，我们定义数据读取的逻辑。以行为单位的数据进行截取，转换及预处理。\n",
    "   - 最后，我们需要将数据整理为特定的格式，才能够被PaddleRec的Reader正确读取，并灌入的训练的网络中。简单来说，数据的输出顺序与我们在网络中创建的`inputs`必须是严格一一对应的，并转换为类似字典的形式。\n",
    "   \n",
    "示例： 假设数据ABC在文本数据中，每行以这样的形式存储：  \n",
    "    ```\n",
    "    0.1,0.2,0.3...3.0,3.1,3.2 \\t 99999,99998,99997 \\t 1 \\n\n",
    "    ```\n",
    "    \n",
    "则示例代码如下：\n",
    "\n",
    " ```python\n",
    "    from paddlerec.core.utils import envs\n",
    "    class Reader(ReaderBase):\n",
    "        def init(self):\n",
    "            self.avg = envs.get_global_env(\"avg\", None, \"hyper_parameters.reader\")\n",
    "\n",
    "        def generator_sample(self, line):\n",
    "            \n",
    "            def reader(self, line):\n",
    "                # 先分割 '\\n'， 再以 '\\t'为标志分割为list\n",
    "                variables = (line.strip('\\n')).split('\\t')\n",
    "\n",
    "                # A是第一个元素，并且每个数据之间使用','分割\n",
    "                var_a = variables[0].split(',') # list\n",
    "                var_a = [float(i) / self.avg for i in var_a] # 将str数据转换为float\n",
    "                \n",
    "\n",
    "                # B是第二个元素，同样以 ',' 分割\n",
    "                var_b = variables[1].split(',') # list\n",
    "                var_b = [int(i) for i in var_b] # 将str数据转换为int\n",
    "\n",
    "                # C是第三个元素, 只有一个元素，没有分割符\n",
    "                var_c = variables[2]\n",
    "                var_c = int(var_c) # 将str数据转换为int\n",
    "                var_c = [var_c] # 将单独的数据元素置入list中\n",
    "\n",
    "                # 将数据与数据名结合，组织为dict的形式\n",
    "                # 如下，output形式为{ A: var_a, B: var_b, C: var_c}\n",
    "                variable_name = ['A', 'B', 'C']\n",
    "                output = zip(variable_name, [var_a] + [var_b] + [var_c])\n",
    "\n",
    "                # 将数据输出，使用yield方法，将该函数变为了一个可迭代的对象\n",
    "                yield output\n",
    "\n",
    " ```\n",
    "    \n",
    "至此，我们完成了Reader的实现。  \n",
    "最后，在配置文件config.yaml中，加入自定义Reader的路径。  \n",
    "```yaml\n",
    "dataset:\n",
    "- name: train_dataset\n",
    "  batch_size: 4096\n",
    "  type: DataLoader # or QueueDataset \n",
    "  data_path: \"{workspace}/data/train\"\n",
    "  data_converter: \"{workspace}/reader.py\"\n",
    "```\n",
    "若要详细了解这种输入方式，点击[这里](https://github.com/PaddlePaddle/PaddleRec/blob/master/doc/custom_reader.md)了解更多  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在本例中，我们使用自定义reader的方式输入数据。 构建的reader如下所示： \n",
    "```python\n",
    "class Reader(ReaderBase):\n",
    "    def init(self):\n",
    "        pass\n",
    "\n",
    "    def generate_sample(self, line):\n",
    "        \"\"\"\n",
    "        Read the data line by line and process it as a dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        def reader():\n",
    "            \"\"\"\n",
    "            This function needs to be implemented by the user, based on data format\n",
    "            \"\"\"\n",
    "            # 将数据中的每一行，先去除结尾的换行，再以逗号分割\n",
    "            features = line.strip('\\n').split(',')\n",
    "            # 每一行中从第一个到倒数第二个是我们构建的特征\n",
    "            data = features[:-1]\n",
    "            data = [float(i) for i in data]\n",
    "            # 每一行中最后一个是label，表示用户最终是否购买\n",
    "            label = features[-1]\n",
    "            label = [int(float(label))]\n",
    "            # 函数应返回一个可以迭代的reader方法\n",
    "            features_name = [\"data\", \"label\"]\n",
    "            yield zip(features_name, [data] + [label])\n",
    "\n",
    "        return reader\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 构建模型\n",
    "如介绍所说，我们需要添加模型文件model.py，在其中定义模型的组网。\n",
    "1. 基类的继承  \n",
    "继承`paddlerec.core.model`的ModelBase，命名为`Class Model`  \n",
    "```python\n",
    "from paddlerec.core.model import ModelBase\n",
    "\n",
    "\n",
    "class Model(ModelBase):\n",
    "\n",
    "    # 构造函数无需显式指定\n",
    "    # 若继承，务必调用基类的__init__方法\n",
    "    def __init__(self, config):\n",
    "        ModelBase.__init__(self, config)\n",
    "        # ModelBase的__init__方法会调用_init_hyper_parameter()\n",
    "```\n",
    "2. 超参的初始化  \n",
    "继承并实现`_init_hyper_parameter`方法(必要)，可以在该方法中，从`yaml`文件获取超参或进行自定义操作。如下面的示例：  \n",
    "所有的envs调用接口在_init_hyper_parameters()方法中实现，同时类成员也推荐在此做声明及初始化。  \n",
    "```python\n",
    "def _init_hyper_parameters(self):\n",
    "    self.fc1_size = envs.get_global_env(\"hyper_parameters.fc1_size\")\n",
    "    self.fc2_size = envs.get_global_env(\"hyper_parameters.fc2_size\")\n",
    "```\n",
    "3. 数据输入的定义\n",
    "继承并实现`input_data`方法  \n",
    "`ModelBase`中的input_data默认实现为slot_reader，在`config.yaml`中分别配置`dataset.sparse_slots`及`dataset.dense_slots`选项实现`slog:feasign`模式的数据读取。配置好了之后，这些slot对应的variable在model中可以使用如下方式调用：  \n",
    "```\n",
    "self._sparse_data_var\n",
    "self._dense_data_var\n",
    "```\n",
    "如果您不想使用`slot:feasign`模式，则需继承并实现`input_data`接口，在模型组网中加入输入占位符。接口定义：`def input_data(self, is_infer=False, **kwargs)`  \n",
    "Reader读取文件后，产出的数据喂入网络，需要有占位符进行接收。占位符在Paddle中使用`fluid.data`或`fluid.layers.data`进行定义。`data`的定义可以参考[fluid.data](https://www.paddlepaddle.org.cn/documentation/docs/zh/api_cn/fluid_cn/data_cn.html#data)以及[fluid.layers.data](https://www.paddlepaddle.org.cn/documentation/docs/zh/api_cn/layers_cn/data_cn.html#data)。\n",
    "```python\n",
    "def input_data(self, is_infer=False, **kwargs):\n",
    "    data = fluid.data(name=\"data\", shape=[None,65], dtype='float32')\n",
    "    label = fluid.data(name=\"label\", shape=[None,1], dtype='int64')\n",
    "    return [data, label]\n",
    "```\n",
    "4. 组网的定义  \n",
    "继承并实现`net`方法(必要)    \n",
    "接口定义`def net(self, inputs, is_infer=False)`  \n",
    "自定义网络需在该函数中使用paddle组网，实现前向逻辑，定义网络的Loss及Metrics，通过`is_infer`判断是否为infer网络。  \n",
    "我们强烈建议`train`及`infer`尽量复用相同代码，`net`中调用的其他函数以下划线为头进行命名，封装网络中的结构模块，如`_sparse_embedding_layer(self)`。  \n",
    "若使用`自定义Reader`方式，`inputs`为`def input_data()`的输出，若使用`slot_reader`方式，inputs为占位符，无实际意义，通过以下示例方法拿到dense及sparse的输入：\n",
    "  ```python\n",
    "  self.sparse_inputs = self._sparse_data_var[1:]\n",
    "  self.dense_input = self._dense_data_var[0]\n",
    "  self.label_input = self._sparse_data_var[0]\n",
    "  ```\n",
    "可以参考官方模型的示例学习net的构造方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "本例中的模型构建：\n",
    "```python\n",
    "import math\n",
    "import paddle.fluid as fluid\n",
    "from paddlerec.core.utils import envs\n",
    "from paddlerec.core.model import ModelBase\n",
    "\n",
    "\n",
    "class Model(ModelBase):\n",
    "    def __init__(self, config):\n",
    "        ModelBase.__init__(self, config)\n",
    "\n",
    "    def _init_hyper_parameters(self):\n",
    "        # 获得超参数，全连接层的输出维度\n",
    "        self.fc1_size = envs.get_global_env(\n",
    "            \"hyper_parameters.fc1_size\")\n",
    "        self.fc2_size = envs.get_global_env(\n",
    "            \"hyper_parameters.fc2_size\")\n",
    "        \n",
    "    def input_data(self, is_infer=False, **kwargs):\n",
    "        # 从自定义的Reader中接收数据\n",
    "        data = fluid.data(name=\"data\", shape=[None,65], dtype='float32')\n",
    "        label = fluid.data(name=\"label\", shape=[None,1], dtype='int64')\n",
    "        return [data, label]\n",
    "\n",
    "    def net(self, inputs, is_infer=False):\n",
    "        # inputs为def input_data()的输出\n",
    "        self.data_inputs = inputs[0]\n",
    "        self.label_inputs = inputs[1]\n",
    "        # 构建组网，三层全连接层，最后softmax的结果即为预测值。\n",
    "        fc1 = fluid.layers.fc(input = self.data_inputs,size = self.fc1_size ,act='relu')\n",
    "        fc1_1 = fluid.layers.batch_norm(input = fc1,act = 'relu')\n",
    "        fc2 = fluid.layers.fc(input = fc1_1 ,size = self.fc2_size ,act='relu')\n",
    "        predict = fluid.layers.fc(input = fc2 ,size=2,act='softmax')\n",
    "        self.predict = predict\n",
    "        # 计算模型的AUC指标\n",
    "        auc, batch_auc, _ = fluid.layers.auc(input=self.predict,\n",
    "                                             label=self.label_inputs,\n",
    "                                             num_thresholds=2**12,\n",
    "                                             slide_steps=20)\n",
    "        # 在执行infer过程时的指标输出\n",
    "        if is_infer:\n",
    "            self._infer_results[\"predict\"] = self.predict\n",
    "            self._infer_results[\"AUC\"] = auc\n",
    "            self._infer_results[\"BATCH_AUC\"] = batch_auc\n",
    "            return\n",
    "        # 在执行train过程时的指标输出\n",
    "        self._metrics[\"AUC\"] = auc\n",
    "        self._metrics[\"BATCH_AUC\"] = batch_auc\n",
    "        # 定义损失函数，计算损失\n",
    "        def wce_loss(pred, label, w=48, epsilon=1e-05):\n",
    "            label = fluid.layers.cast(x=label, dtype=\"float32\")\n",
    "            label = fluid.layers.clip(label, epsilon, 1-epsilon)\n",
    "            pred = fluid.layers.slice(pred,[1],[1],[2])\n",
    "            pred = fluid.layers.clip(pred, epsilon, 1-epsilon)\n",
    "\n",
    "            loss = -1 * (w * label * fluid.layers.log(pred) + (1 - label) * fluid.layers.log(1 - pred))\n",
    "            loss = fluid.layers.reduce_mean(loss)\n",
    "            return loss\n",
    "\n",
    "        avg_cost = wce_loss(self.predict, self.label_input)\n",
    "        self._cost = avg_cost\n",
    "        \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 构建配置文件config.yaml\n",
    "\n",
    "config.yaml中存放着模型的各种配置。其中又大体分为几个模块：\n",
    "\n",
    "\n",
    "- workspace  指定model/reader/data所在位置\n",
    "\n",
    "- dataset  指定数据输入的具体方式\n",
    "\n",
    "- hyper_parameters  模型中需要用到的超参数\n",
    "\n",
    "- mode  指定当次运行使用哪些runner\n",
    "\n",
    "- runner&phase  指定运行的具体方式和参数\n",
    "\n",
    "\n",
    "更加具体的参数请点击[这里](https://github.com/PaddlePaddle/PaddleRec/blob/master/doc/yaml.md)查看更加详细的教程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "creating /tmp/tmpopaw8t6k/paddle_rec.egg-info\n",
      "writing /tmp/tmpopaw8t6k/paddle_rec.egg-info/PKG-INFO\n",
      "writing dependency_links to /tmp/tmpopaw8t6k/paddle_rec.egg-info/dependency_links.txt\n",
      "writing requirements to /tmp/tmpopaw8t6k/paddle_rec.egg-info/requires.txt\n",
      "writing top-level names to /tmp/tmpopaw8t6k/paddle_rec.egg-info/top_level.txt\n",
      "writing manifest file '/tmp/tmpopaw8t6k/paddle_rec.egg-info/SOURCES.txt'\n",
      "reading manifest file '/tmp/tmpopaw8t6k/paddle_rec.egg-info/SOURCES.txt'\n",
      "writing manifest file '/tmp/tmpopaw8t6k/paddle_rec.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec\n",
      "copying build/lib/paddlerec/run.py -> build/bdist.linux-x86_64/egg/paddlerec\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/tools\n",
      "copying build/lib/paddlerec/tools/k8s_tools.py -> build/bdist.linux-x86_64/egg/paddlerec/tools\n",
      "copying build/lib/paddlerec/tools/cal_pos_neg.py -> build/bdist.linux-x86_64/egg/paddlerec/tools\n",
      "copying build/lib/paddlerec/tools/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/tools\n",
      "copying build/lib/paddlerec/tools/tools.py -> build/bdist.linux-x86_64/egg/paddlerec/tools\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/core\n",
      "copying build/lib/paddlerec/core/trainer.py -> build/bdist.linux-x86_64/egg/paddlerec/core\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/core/metrics\n",
      "copying build/lib/paddlerec/core/metrics/precision_recall.py -> build/bdist.linux-x86_64/egg/paddlerec/core/metrics\n",
      "copying build/lib/paddlerec/core/metrics/auc.py -> build/bdist.linux-x86_64/egg/paddlerec/core/metrics\n",
      "copying build/lib/paddlerec/core/metrics/pairwise_pn.py -> build/bdist.linux-x86_64/egg/paddlerec/core/metrics\n",
      "copying build/lib/paddlerec/core/metrics/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/core/metrics\n",
      "copying build/lib/paddlerec/core/metrics/recall_k.py -> build/bdist.linux-x86_64/egg/paddlerec/core/metrics\n",
      "copying build/lib/paddlerec/core/layer.py -> build/bdist.linux-x86_64/egg/paddlerec/core\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/core/trainers\n",
      "copying build/lib/paddlerec/core/trainers/general_trainer.py -> build/bdist.linux-x86_64/egg/paddlerec/core/trainers\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework\n",
      "copying build/lib/paddlerec/core/trainers/framework/instance.py -> build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework\n",
      "copying build/lib/paddlerec/core/trainers/framework/network.py -> build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework\n",
      "copying build/lib/paddlerec/core/trainers/framework/terminal.py -> build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework\n",
      "copying build/lib/paddlerec/core/trainers/framework/runner.py -> build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework\n",
      "copying build/lib/paddlerec/core/trainers/framework/startup.py -> build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework\n",
      "copying build/lib/paddlerec/core/trainers/framework/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework\n",
      "copying build/lib/paddlerec/core/trainers/framework/dataset.py -> build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework\n",
      "copying build/lib/paddlerec/core/trainers/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/core/trainers\n",
      "copying build/lib/paddlerec/core/trainers/finetuning_trainer.py -> build/bdist.linux-x86_64/egg/paddlerec/core/trainers\n",
      "copying build/lib/paddlerec/core/metric.py -> build/bdist.linux-x86_64/egg/paddlerec/core\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/core/utils\n",
      "copying build/lib/paddlerec/core/utils/fs.py -> build/bdist.linux-x86_64/egg/paddlerec/core/utils\n",
      "copying build/lib/paddlerec/core/utils/dataset_holder.py -> build/bdist.linux-x86_64/egg/paddlerec/core/utils\n",
      "copying build/lib/paddlerec/core/utils/validation.py -> build/bdist.linux-x86_64/egg/paddlerec/core/utils\n",
      "copying build/lib/paddlerec/core/utils/envs.py -> build/bdist.linux-x86_64/egg/paddlerec/core/utils\n",
      "copying build/lib/paddlerec/core/utils/dataloader_instance.py -> build/bdist.linux-x86_64/egg/paddlerec/core/utils\n",
      "copying build/lib/paddlerec/core/utils/dataset_instance.py -> build/bdist.linux-x86_64/egg/paddlerec/core/utils\n",
      "copying build/lib/paddlerec/core/utils/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/core/utils\n",
      "copying build/lib/paddlerec/core/utils/util.py -> build/bdist.linux-x86_64/egg/paddlerec/core/utils\n",
      "copying build/lib/paddlerec/core/utils/table.py -> build/bdist.linux-x86_64/egg/paddlerec/core/utils\n",
      "copying build/lib/paddlerec/core/reader.py -> build/bdist.linux-x86_64/egg/paddlerec/core\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/core/modules\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/core/modules/coding\n",
      "copying build/lib/paddlerec/core/modules/coding/layers.py -> build/bdist.linux-x86_64/egg/paddlerec/core/modules/coding\n",
      "copying build/lib/paddlerec/core/modules/coding/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/core/modules/coding\n",
      "copying build/lib/paddlerec/core/modules/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/core/modules\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/core/modules/modul\n",
      "copying build/lib/paddlerec/core/modules/modul/layers.py -> build/bdist.linux-x86_64/egg/paddlerec/core/modules/modul\n",
      "copying build/lib/paddlerec/core/modules/modul/build.py -> build/bdist.linux-x86_64/egg/paddlerec/core/modules/modul\n",
      "copying build/lib/paddlerec/core/modules/modul/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/core/modules/modul\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/core/engine\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster\n",
      "copying build/lib/paddlerec/core/engine/cluster/cluster.py -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/cloud\n",
      "copying build/lib/paddlerec/core/engine/cluster/cloud/k8s_cpu_job.sh.template -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/cloud\n",
      "copying build/lib/paddlerec/core/engine/cluster/cloud/before_hook_cpu.sh.template -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/cloud\n",
      "copying build/lib/paddlerec/core/engine/cluster/cloud/before_hook_gpu.sh.template -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/cloud\n",
      "copying build/lib/paddlerec/core/engine/cluster/cloud/mpi_config.ini.template -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/cloud\n",
      "copying build/lib/paddlerec/core/engine/cluster/cloud/k8s_job.sh.template -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/cloud\n",
      "copying build/lib/paddlerec/core/engine/cluster/cloud/cluster.sh -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/cloud\n",
      "copying build/lib/paddlerec/core/engine/cluster/cloud/mpi_job.sh.template -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/cloud\n",
      "copying build/lib/paddlerec/core/engine/cluster/cloud/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/cloud\n",
      "copying build/lib/paddlerec/core/engine/cluster/cloud/k8s_config.ini.template -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/cloud\n",
      "copying build/lib/paddlerec/core/engine/cluster/cloud/end_hook.sh.template -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/cloud\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/k8s\n",
      "copying build/lib/paddlerec/core/engine/cluster/k8s/set_k8s_env.sh -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/k8s\n",
      "copying build/lib/paddlerec/core/engine/cluster/k8s/cluster.sh -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/k8s\n",
      "copying build/lib/paddlerec/core/engine/cluster/k8s/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/k8s\n",
      "copying build/lib/paddlerec/core/engine/cluster/k8s/k8s.yaml.template -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/k8s\n",
      "copying build/lib/paddlerec/core/engine/cluster/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster\n",
      "copying build/lib/paddlerec/core/engine/engine.py -> build/bdist.linux-x86_64/egg/paddlerec/core/engine\n",
      "copying build/lib/paddlerec/core/engine/local_mpi.py -> build/bdist.linux-x86_64/egg/paddlerec/core/engine\n",
      "copying build/lib/paddlerec/core/engine/cluster_utils.py -> build/bdist.linux-x86_64/egg/paddlerec/core/engine\n",
      "copying build/lib/paddlerec/core/engine/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/core/engine\n",
      "copying build/lib/paddlerec/core/engine/local_cluster.py -> build/bdist.linux-x86_64/egg/paddlerec/core/engine\n",
      "copying build/lib/paddlerec/core/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/core\n",
      "copying build/lib/paddlerec/core/factory.py -> build/bdist.linux-x86_64/egg/paddlerec/core\n",
      "copying build/lib/paddlerec/core/model.py -> build/bdist.linux-x86_64/egg/paddlerec/core\n",
      "copying build/lib/paddlerec/setup.py -> build/bdist.linux-x86_64/egg/paddlerec\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/tests\n",
      "copying build/lib/paddlerec/tests/test_auc_metrics.py -> build/bdist.linux-x86_64/egg/paddlerec/tests\n",
      "copying build/lib/paddlerec/tests/test_recall_k.py -> build/bdist.linux-x86_64/egg/paddlerec/tests\n",
      "copying build/lib/paddlerec/tests/test_precision_recall_metrics.py -> build/bdist.linux-x86_64/egg/paddlerec/tests\n",
      "copying build/lib/paddlerec/tests/test_pairwise_pn.py -> build/bdist.linux-x86_64/egg/paddlerec/tests\n",
      "copying build/lib/paddlerec/tests/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/tests\n",
      "copying build/lib/paddlerec/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec\n",
      "creating build/bdist.linux-x86_64/egg/paddlerec/doc\n",
      "copying build/lib/paddlerec/doc/markdown2rst.py -> build/bdist.linux-x86_64/egg/paddlerec/doc\n",
      "copying build/lib/paddlerec/doc/__init__.py -> build/bdist.linux-x86_64/egg/paddlerec/doc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/run.py to run.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/tools/k8s_tools.py to k8s_tools.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/tools/cal_pos_neg.py to cal_pos_neg.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/tools/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/tools/tools.py to tools.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/trainer.py to trainer.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/metrics/precision_recall.py to precision_recall.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/metrics/auc.py to auc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/metrics/pairwise_pn.py to pairwise_pn.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/metrics/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/metrics/recall_k.py to recall_k.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/layer.py to layer.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/trainers/general_trainer.py to general_trainer.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework/instance.py to instance.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework/network.py to network.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework/terminal.py to terminal.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework/runner.py to runner.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework/startup.py to startup.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/trainers/framework/dataset.py to dataset.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/trainers/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/trainers/finetuning_trainer.py to finetuning_trainer.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/metric.py to metric.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/utils/fs.py to fs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/utils/dataset_holder.py to dataset_holder.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/utils/validation.py to validation.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/utils/envs.py to envs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/utils/dataloader_instance.py to dataloader_instance.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/utils/dataset_instance.py to dataset_instance.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/utils/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/utils/util.py to util.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/utils/table.py to table.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/reader.py to reader.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/modules/coding/layers.py to layers.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/modules/coding/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/modules/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/modules/modul/layers.py to layers.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/modules/modul/build.py to build.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/modules/modul/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/cluster.py to cluster.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/cloud/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/k8s/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/engine/engine.py to engine.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/engine/local_mpi.py to local_mpi.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/engine/cluster_utils.py to cluster_utils.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/engine/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/engine/local_cluster.py to local_cluster.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/factory.py to factory.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/core/model.py to model.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/setup.py to setup.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/tests/test_auc_metrics.py to test_auc_metrics.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/tests/test_recall_k.py to test_recall_k.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/tests/test_precision_recall_metrics.py to test_precision_recall_metrics.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/tests/test_pairwise_pn.py to test_pairwise_pn.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/tests/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/doc/markdown2rst.py to markdown2rst.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/paddlerec/doc/__init__.py to __init__.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying /tmp/tmpopaw8t6k/paddle_rec.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying /tmp/tmpopaw8t6k/paddle_rec.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying /tmp/tmpopaw8t6k/paddle_rec.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying /tmp/tmpopaw8t6k/paddle_rec.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying /tmp/tmpopaw8t6k/paddle_rec.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying /tmp/tmpopaw8t6k/paddle_rec.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "creating 'dist/paddle_rec-1.8.5.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing paddle_rec-1.8.5.1-py3.7.egg\n",
      "creating /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle_rec-1.8.5.1-py3.7.egg\n",
      "Extracting paddle_rec-1.8.5.1-py3.7.egg to /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages\n",
      "Adding paddle-rec 1.8.5.1 to easy-install.pth file\n",
      "\n",
      "Installed /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle_rec-1.8.5.1-py3.7.egg\n",
      "Processing dependencies for paddle-rec==1.8.5.1\n",
      "Searching for PyYAML==5.1.2\n",
      "Best match: PyYAML 5.1.2\n",
      "Adding PyYAML 5.1.2 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages\n",
      "Finished processing dependencies for paddle-rec==1.8.5.1\n",
      "\n",
      "Installation Complete. Congratulations!\n",
      "How to use it ? Please visit our webside: https://github.com/PaddlePaddle/PaddleRec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 环境部署\r\n",
    "# 安装PaddleRec\r\n",
    "!cd PaddleRec/ && python setup.py install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘PaddleRec/models/demo/competition/data’: File exists\n",
      "mkdir: cannot create directory ‘PaddleRec/models/demo/competition/data/train’: File exists\n",
      "mkdir: cannot create directory ‘PaddleRec/models/demo/competition/data/valid’: File exists\n",
      "mkdir: cannot create directory ‘PaddleRec/models/demo/competition/data/test’: File exists\n"
     ]
    }
   ],
   "source": [
    "# 将之前准备好的数据移到相应的目录下\r\n",
    "!mkdir PaddleRec/models/demo/competition/data\r\n",
    "!mkdir PaddleRec/models/demo/competition/data/train\r\n",
    "!mkdir PaddleRec/models/demo/competition/data/valid\r\n",
    "!mkdir PaddleRec/models/demo/competition/data/test\r\n",
    "!mv train_1.csv PaddleRec/models/demo/competition/data/train\r\n",
    "!mv valid_1.csv PaddleRec/models/demo/competition/data/valid\r\n",
    "!mv test_1.csv PaddleRec/models/demo/competition/data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "                Runtime Envs                      Value                                             \n",
      "----------------------------------------------------------------------------------------------------\n",
      "train.trainer.trainer                             GeneralTrainer                                    \n",
      "train.trainer.executor_mode                       train                                             \n",
      "train.trainer.threads                             2                                                 \n",
      "train.trainer.platform                            LINUX                                             \n",
      "train.trainer.engine                              single                                            \n",
      "====================================================================================================\n",
      "\n",
      "QueueDataset can not support PY3, change to DataLoader\n",
      "\n",
      "========================================================================================================\n",
      "              paddlerec Global Envs                   Value                                             \n",
      "--------------------------------------------------------------------------------------------------------\n",
      "workspace                                             ./                                                \n",
      "dataset.train_dataset.name                            train_dataset                                     \n",
      "dataset.train_dataset.batch_size                      4096                                              \n",
      "dataset.train_dataset.type                            DataLoader                                        \n",
      "dataset.train_dataset.data_path                       .//data/train                                     \n",
      "dataset.train_dataset.data_converter                  .//reader.py                                      \n",
      "hyper_parameters.optimizer.class                      Adam                                              \n",
      "hyper_parameters.optimizer.learning_rate              0.0006                                            \n",
      "hyper_parameters.optimizer.strategy                   async                                             \n",
      "hyper_parameters.fc1_size                             128                                               \n",
      "hyper_parameters.fc2_size                             128                                               \n",
      "mode                                                  ['train_runner']                                  \n",
      "runner.train_runner.name                              train_runner                                      \n",
      "runner.train_runner.class                             train                                             \n",
      "runner.train_runner.epochs                            32                                                \n",
      "runner.train_runner.device                            cpu                                               \n",
      "runner.train_runner.save_checkpoint_interval          1                                                 \n",
      "runner.train_runner.save_inference_interval           1                                                 \n",
      "runner.train_runner.save_checkpoint_path              increment                                         \n",
      "runner.train_runner.save_inference_path               inference                                         \n",
      "runner.train_runner.save_inference_feed_varnames      []                                                \n",
      "runner.train_runner.save_inference_fetch_varnames     []                                                \n",
      "runner.train_runner.init_model_path                                                                     \n",
      "runner.train_runner.print_interval                    100                                               \n",
      "runner.train_runner.phases                            phase_train                                       \n",
      "phase.phase_train.name                                phase_train                                       \n",
      "phase.phase_train.model                               .//model.py                                       \n",
      "phase.phase_train.dataset_name                        train_dataset                                     \n",
      "phase.phase_train.thread_num                          1                                                 \n",
      "========================================================================================================\n",
      "\n",
      "PaddleRec: Runner train_runner Begin\n",
      "Executor Mode: train\n",
      "processor_register begin\n",
      "Running SingleInstance.\n",
      "Running SingleNetwork.\n",
      "Warning:please make sure there are no hidden files in the dataset folder and check these hidden files:[]\n",
      "need_split_files: False\n",
      "QueueDataset can not support PY3, change to DataLoader\n",
      "QueueDataset can not support PY3, change to DataLoader\n",
      "Running SingleStartup.\n",
      "Running SingleRunner.\n",
      "!!! The CPU_NUM is not specified, you should set CPU_NUM in the environment variable list.\n",
      "CPU_NUM indicates that how many CPUPlace are used in the current task.\n",
      "And if this parameter are set as N (equal to the number of physical CPU core) the program may be faster.\n",
      "\n",
      "export CPU_NUM=24 # for example, set CPU_NUM as number of physical CPU core which is 24.\n",
      "\n",
      "!!! The default number of CPU_NUM=1.\n",
      "2021-06-03 09:24:42,819-INFO: \t[Train],  epoch: 0,  batch: 100, time_each_interval: 10.42s, AUC: [0.66074147], BATCH_AUC: [0.69605992]\n",
      "2021-06-03 09:24:53,243-INFO: \t[Train],  epoch: 0,  batch: 200, time_each_interval: 10.42s, AUC: [0.66576546], BATCH_AUC: [0.66999205]\n",
      "2021-06-03 09:25:03,299-INFO: \t[Train],  epoch: 0,  batch: 300, time_each_interval: 10.06s, AUC: [0.67946585], BATCH_AUC: [0.67411291]\n",
      "2021-06-03 09:25:14,278-INFO: \t[Train],  epoch: 0,  batch: 400, time_each_interval: 10.98s, AUC: [0.68791321], BATCH_AUC: [0.69761034]\n",
      "2021-06-03 09:25:25,180-INFO: \t[Train],  epoch: 0,  batch: 500, time_each_interval: 10.90s, AUC: [0.6963851], BATCH_AUC: [0.65210833]\n",
      "epoch 0 done, use time: 56.430397748947144, global metrics: AUC=[0.70272092], BATCH_AUC=[0.69047335]\n",
      "2021-06-03 09:25:28,801-INFO: \tsave epoch_id:0 model into: \"increment/0\"\n",
      "2021-06-03 09:25:39,228-INFO: \t[Train],  epoch: 1,  batch: 100, time_each_interval: 10.41s, AUC: [0.70235607], BATCH_AUC: [0.70325724]\n",
      "2021-06-03 09:25:49,419-INFO: \t[Train],  epoch: 1,  batch: 200, time_each_interval: 10.19s, AUC: [0.70143481], BATCH_AUC: [0.66819462]\n",
      "2021-06-03 09:25:59,627-INFO: \t[Train],  epoch: 1,  batch: 300, time_each_interval: 10.21s, AUC: [0.70233338], BATCH_AUC: [0.67513684]\n",
      "2021-06-03 09:26:09,994-INFO: \t[Train],  epoch: 1,  batch: 400, time_each_interval: 10.27s, AUC: [0.70265872], BATCH_AUC: [0.69863304]\n",
      "2021-06-03 09:26:20,286-INFO: \t[Train],  epoch: 1,  batch: 500, time_each_interval: 10.39s, AUC: [0.70502093], BATCH_AUC: [0.66902445]\n",
      "epoch 1 done, use time: 54.9131498336792, global metrics: AUC=[0.70776657], BATCH_AUC=[0.69042762]\n",
      "2021-06-03 09:26:23,734-INFO: \tsave epoch_id:1 model into: \"increment/1\"\n",
      "2021-06-03 09:26:34,906-INFO: \t[Train],  epoch: 2,  batch: 100, time_each_interval: 11.15s, AUC: [0.70729557], BATCH_AUC: [0.70832706]\n",
      "2021-06-03 09:26:46,063-INFO: \t[Train],  epoch: 2,  batch: 200, time_each_interval: 11.16s, AUC: [0.70684559], BATCH_AUC: [0.67299251]\n",
      "2021-06-03 09:26:57,373-INFO: \t[Train],  epoch: 2,  batch: 300, time_each_interval: 11.31s, AUC: [0.70733364], BATCH_AUC: [0.6828149]\n",
      "2021-06-03 09:27:08,648-INFO: \t[Train],  epoch: 2,  batch: 400, time_each_interval: 11.28s, AUC: [0.70714998], BATCH_AUC: [0.69917374]\n",
      "2021-06-03 09:27:20,020-INFO: \t[Train],  epoch: 2,  batch: 500, time_each_interval: 11.37s, AUC: [0.70837348], BATCH_AUC: [0.67133144]\n",
      "epoch 2 done, use time: 60.070722818374634, global metrics: AUC=[0.71011642], BATCH_AUC=[0.69110736]\n",
      "2021-06-03 09:27:23,824-INFO: \tsave epoch_id:2 model into: \"increment/2\"\n",
      "2021-06-03 09:27:35,007-INFO: \t[Train],  epoch: 3,  batch: 100, time_each_interval: 11.16s, AUC: [0.70978824], BATCH_AUC: [0.71258892]\n",
      "2021-06-03 09:27:45,553-INFO: \t[Train],  epoch: 3,  batch: 200, time_each_interval: 10.55s, AUC: [0.70955047], BATCH_AUC: [0.66975988]\n",
      "2021-06-03 09:27:55,756-INFO: \t[Train],  epoch: 3,  batch: 300, time_each_interval: 10.20s, AUC: [0.70996209], BATCH_AUC: [0.68240173]\n",
      "2021-06-03 09:28:05,934-INFO: \t[Train],  epoch: 3,  batch: 400, time_each_interval: 10.18s, AUC: [0.70968469], BATCH_AUC: [0.69662006]\n",
      "2021-06-03 09:28:16,156-INFO: \t[Train],  epoch: 3,  batch: 500, time_each_interval: 10.22s, AUC: [0.71052378], BATCH_AUC: [0.67098774]\n",
      "epoch 3 done, use time: 55.79302096366882, global metrics: AUC=[0.71177477], BATCH_AUC=[0.69168165]\n",
      "2021-06-03 09:28:19,637-INFO: \tsave epoch_id:3 model into: \"increment/3\"\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\r\n",
    "# 模型训练10个epoch\r\n",
    "!cd PaddleRec/models/demo/competition && python -m paddlerec.run -m ./config_train.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 生成提交文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在config_train.yaml中可以设定训练时参数文件的保存目录。  \n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7811651b34c946acac3f088b2ed0cfc5d11647cf30514ec0affc61c4cf529324)  \n",
    "进入这个目录，可以看到其中保存了0到9共10个目录。这就是训练时保存下来的参数文件。我们可以选择其中一个目录用来初始化模型进行预测。  \n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/2e185d2dc3f24c08aab3a89d21ca798888f8a5a0111e471e9cc1799c34e5a646)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开始预测，并将结果重定向到infer_log.txt文件中\r\n",
    "!cd PaddleRec/models/demo/competition && python -m paddlerec.run -m ./config_infer.yaml 2>log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#只保留有“predict”的行，也就是有预测值的行\r\n",
    "!mv PaddleRec/models/demo/competition/log.txt ./\r\n",
    "!grep -i \"predict\" ./log.txt > ./result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#将预测结果与submission.csv组合起来\r\n",
    "filename = './result.txt'\r\n",
    "prediction = [0]\r\n",
    "for line in open(filename):\r\n",
    "    line = line.strip().split(\",\")\r\n",
    "    line[3] = line[3].split(\":\")\r\n",
    "    line = line[3][1].strip(\" \")\r\n",
    "    line = line.strip(\"[]\").split(\" \")\r\n",
    "    if float(line[0])>0.5:\r\n",
    "        prediction.append(0)\r\n",
    "    else:\r\n",
    "        prediction.append(1)\r\n",
    "\r\n",
    "filename = './test_customer_id.csv'\r\n",
    "customer_id = []\r\n",
    "for line in open(filename):\r\n",
    "    line = line.strip()\r\n",
    "    customer_id.append(int(line))\r\n",
    "\r\n",
    "id_value = dict(zip(customer_id,prediction))\r\n",
    "\r\n",
    "filename = 'data/data19383/submission.csv'\r\n",
    "f = open(filename, \"r\")\r\n",
    "f.readline()\r\n",
    "for line in f.readlines():\r\n",
    "    line = line.strip().split(\",\")\r\n",
    "    id_value.setdefault(int(float(line[0])) , 0)\r\n",
    "\r\n",
    "sub_list = list(id_value.items())\r\n",
    "sub_list = sorted(sub_list)\r\n",
    "submission = pd.DataFrame(columns=['customer_id',\"result\"],data = sub_list)\r\n",
    "submission.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 写在最后\n",
    "\n",
    "### 可选优化方案\n",
    "\n",
    "> 本次比赛可调优空间非常大，可尝试且不限于从以下方面来进行调优，如果尝试后发现效果并不理想，可以在基线项目的评论区中和大家一起讨论~\n",
    "\n",
    "**数据处理**\n",
    "\n",
    "> 1. 归一化方案 - 直接拉伸是最佳方式吗？\n",
    "> 2. 离散值与连续值 - 哪种方式更适合处理这些方式？是否有较为通用的方法可以尝试？是否可以使用Embedding？\n",
    "> 3. 特征工程 - 除了时间滑窗是否可以有其它特征？有没有不使用特征工程的解决方案？\n",
    "> 4. 特征选择 - 输入特征真的是越多越好吗？如何选择特征以克服神经网络训练的不稳定性？\n",
    "> 5. 数据集划分比例 - 训练集、验证集、测试集应该怎样划分效果更好？\n",
    "\n",
    "**首层网络选择**\n",
    "\n",
    "> 1. Embedding还是Linear、Conv？- 如果使用卷积应该怎样处理shape？\n",
    "> 2. 多字段合并输入还是分开输入？- 分开输入效果一定好吗？哪些字段更适合合并输入？\n",
    "\n",
    "**网络(Backbone)部分搭建**\n",
    "\n",
    "> 1. 隐层大小选择 - 宽度和层数\n",
    "> 2. 尝试复杂网络构建 - 是否可以尝试简单CNN、RNN？如何使用飞桨复现经典解决方案？是否可以尝试使用图神经网络？如何使用PGL构建本赛题的异构图？\n",
    "> 3. 选择更合适的激活函数\n",
    "> 4. 尝试正则化、dropout等方式避免过拟合\n",
    "> 5. 尝试非Xavier初始化方案\n",
    "\n",
    "**模型(Model)搭建以及训练相关**\n",
    "\n",
    "> 1. 选择学习率等超参数\n",
    "> 2. 选择合适的损失函数 - 如何处理数据不平衡问题？\n",
    "> 3. 尝试不同的优化器\n",
    "> 4. 尝试使用学习率调度器\n",
    "> 5. 避免脏数据干扰(用深度学习的方式更优更方便)\n",
    "\n",
    "**模型融合**\n",
    "\n",
    "> 1. 深度学习模型自身是否需要进行模型融合？模型融合是否能克服神经网络训练的不稳定性？\n",
    "> 2. 是否能使用不同深度学习模型进行融合？\n",
    "\n",
    "**提交相关**\n",
    "\n",
    "> 1. 测试集表现最好的模型一定是最优秀的吗？\n",
    "> 2. 用准确率来衡量二分类模型的能力是最好选择吗？\n",
    "\n",
    "### 参考资料\n",
    "\n",
    "- 用户购买预测练习赛总结\n",
    "  - [用户购买预测练习赛：数据集探索分析（EDA）](https://aistudio.baidu.com/aistudio/projectdetail/438644)\n",
    "\n",
    "      - 介绍时间序列数据常见的探索思路在AI Studio上的实现\n",
    "\n",
    "  - [用户购买预测练习赛：时间滑窗特征构建](https://aistudio.baidu.com/aistudio/projectdetail/276829)\n",
    "\n",
    "      - 介绍时间序列数据上时间滑窗的特征的生成与内存优化策略\n",
    "\n",
    "  - [用户购买预测练习赛：用户和产品特征构建](https://aistudio.baidu.com/aistudio/projectdetail/438772)\n",
    "\n",
    "      - 介绍基本推荐算法的特征工程在比赛数据集上的实现\n",
    "\n",
    "  - [用户购买预测练习赛：月销量预测实现](https://aistudio.baidu.com/aistudio/projectdetail/438793)\n",
    "\n",
    "      - 另一种时间滑窗思路，按月预测销量，属于分支探索\n",
    "- [连锁超市销量预估案例](https://www.kaggle.com/c/favorita-grocery-sales-forecasting)\n",
    "  - [数据集已上传到AI Studio平台](https://aistudio.baidu.com/aistudio/datasetdetail/17815)\n",
    "  - [第一名解决方案](https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47582)\n",
    "  - [第五名解决方案](https://github.com/LenzDu/Kaggle-Competition-Favorita)\n",
    "- [月销量预测案例](https://www.kaggle.com/c/competitive-data-science-predict-future-sales)\n",
    "- [一套系统的特征工程框架](https://github.com/dayeren/Kaggle_Competition_Treasure/tree/master/Recommendations/Instacart)\n",
    "\n",
    "## 代码审查\n",
    "\n",
    "如选手成绩靠前并收到官方邮件通知代码审查，请参考该[链接](https://aistudio.baidu.com/aistudio/projectdetail/743661)进行项目上传操作  \n",
    "快捷命令:`!zip -rP [此处添加审查邮件中的Key值] [邮件中的UID值].zip /home/aistudio/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.8.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
